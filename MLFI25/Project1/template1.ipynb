{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0864f6f8-d36b-4bb2-9a39-ce2e9780c5f4",
   "metadata": {},
   "source": [
    "# Coding Project 1 : Linear Regression and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b0c7d-92b3-4cde-a31b-93ff1e7ff46f",
   "metadata": {},
   "source": [
    "**Tristan Þórðarson**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "*Note:* The provided structure for the code below is only suggestive, and if you want to structure your programs differently you may do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87ace5-781a-402a-8f0b-3266c446cd30",
   "metadata": {},
   "source": [
    "### Question 1 - Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf5dd6ed-bdad-4351-a2ff-01aded150b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Question 1, you can import the following packages:\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection  import train_test_split\n",
    "\n",
    "#1.a) Import the dataset Housing.csv into Python as a pandas DataFrame.\n",
    "#To determine whether a variable is categorical or numerical, please refer to the file data_description.txt.\n",
    "#Remember that the first column of the csv file is an index column and should not be considered as an explanatory variable.\n",
    "\n",
    "\n",
    "#1.b) Graphically determine whether the target variable SalePrice is approximately Gaussian.\n",
    "\n",
    "#If not, suggest a suitable transformation to bring SalePrice closer to a Gaussian distribution (logarithmic, inverse, square-root, ...).\n",
    "\n",
    "#Do not forget to apply this transformation to the target variable SalePrice in the dataset.\n",
    "\n",
    "#Why is it important to consider such potential transformations?\n",
    "\n",
    "\n",
    "#1.c) Split the data into a training set (X,y)_train and a test set (X,y)_test.\n",
    "#Randomly assign 70% of the observations to the training set and the remaining 30% to the test set.\n",
    "\n",
    "\n",
    "#1.d) Replace missing values in X using the training data statistics only -> use .fillna(...)\n",
    "#For numerical features, replace missing values with the mean of the column .\n",
    "#For categorical features, replace missing values with the most frequent category.\n",
    "#You can use the function df.select_dtypes(...) to idetify categorical variables as the variables with type 'object' and 'category'. \n",
    "#Some categorical variables admit NA (or None) as a valid category, which should be treated as an actual level and not as missing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86354e30-0b82-481b-a39f-c611e08a4408",
   "metadata": {},
   "source": [
    "### Question 2 - Linear Regression on Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1114545-6d94-4276-9e6a-cc84a49309bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#2.a) Fit a linear regression model on the training dataset with numerical features only using the sklearn package.\n",
    "#Output a table with the name of each feature and the associated regression coefficient.\n",
    "\n",
    "#Compare the in-sample and out-of-sample Mean Squared Error (MSE) and R^2.\n",
    "\n",
    "\n",
    "#2.b) (i) Compute the estimated coefficients for each explanatory variable using the numpy package. \n",
    "#Remember that the design matrix A needs to include a column of 1's. Use np.linalg.solve(...).\n",
    "\n",
    "# (ii) Compute the standard error of each estimated coefficient using the numpy package.\n",
    "\n",
    "# (iii) Compute the (in-sample) MSE and R^2 using the numpy package.\n",
    "\n",
    "# (iv) Do the results change using pseudoinversion instead of standard matrix inversion? Use np.linalg.pinv(...).\n",
    "\n",
    "# (v) Compare your results (with and without pseudoinversion) to the output of the sm.OLS(...) function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31c5ecc-7f64-4788-9237-76dff4e3fb3e",
   "metadata": {},
   "source": [
    "### Question 3 - Regularization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12435fe-6e93-477f-ac34-3440e08463f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold, KFold\n",
    "\n",
    "# 3.a) Linear regression with the full Housing dataset (numerical + categorical).\n",
    "#How do the in-sample and out-of-sample metrics change when including categorical features, and what does this reveal about the model’s ability to generalize?\n",
    "\n",
    "\n",
    "#3.b) Implement the Truncated Pseudoinverse, Ridge, Lasso, and Elastic Net regularization techniques.\n",
    "#Use 8-fold cross-validation to tune the hyperparameters of each regularization technique based on the MSE metric.\n",
    "#Why is it important that the intercept is not penalized in these models?\n",
    "\n",
    "#Compare their performance in terms of in-sample and out-of-sample MSE and R2 with the linear regressions of Questions 2.a) and 3.a).\n",
    "\n",
    "\n",
    "#3.c) During cross-validation, what are possible sources of information leakage? \n",
    "#Briefly describe what leakage means in this context, and explain what steps you would take to avoid it if you were building a more complete data preprocessing pipeline (e.g., with imputation, scaling, or encoding). \n",
    "#You do not need to implement these steps here, only to explain the idea.\n",
    "\n",
    "\n",
    "#3.d) For the  Lasso and Elastic Net regressions, how many coefficients are non-zero?\n",
    "\n",
    "#Compare this number with the number of coefficients retained by the Ridge and Truncated Pseudoinverse models and provide an explanation.\n",
    "\n",
    "\n",
    "#3.e) #Based on your findings from Questions 2 and 3, which model would you recommend for predicting house prices? \n",
    "#Justify your choice not only by comparing performance metrics, but also by discussing the nature of the problem (e.g., number of features, presence of categorical variables, potential collinearity, sparsity, nonlinearity). \n",
    "#Explain how the strengths and limitations of the chosen method align with this problem structure.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
