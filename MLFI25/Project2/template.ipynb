{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVNblDBpVoAq"
   },
   "source": [
    "**Names of all group members:**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "All code below is only suggestive and you may as well use different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "K1qPukjnSRgc"
   },
   "outputs": [],
   "source": [
    "# Exercise 1.\n",
    "import numpy as np\n",
    "np.random.seed(0)  # for reproducibility\n",
    "\n",
    "# simulate explanatory variables x (the age and monthly income are assumed here to follow a continous uniform distribution)\n",
    "m = 20_000\n",
    "n = 10_000\n",
    "\n",
    "a1,b1 = 18,80\n",
    "x1 = np.random.uniform(a1,b1,m+n) # age\n",
    "\n",
    "a2,b2 = 1,15\n",
    "x2 = np.random.uniform(a2,b2,m+n) # monthly income in 1'000 CHF\n",
    "\n",
    "p = 0.1\n",
    "x3 = np.random.binomial(1,p,m+n) # salaried/self-employed with x3~bern(p) with p = 0.1\n",
    "\n",
    "x1_subs = x1[0:m]\n",
    "x2_subs = x2[0:m]\n",
    "x3_subs = x3[0:m]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly derive the first two moments for comparison purposes.\n",
    "\n",
    "Suppose $Y \\sim Uniform(a,b)$. Then\n",
    "\n",
    "$$\\mathbb{E}\\left[Y\\right] = \\int_{a}^{b} y \\frac{1}{b-a} \\,dy = \\frac{b^2-a^2}{2(b-a)} = \\frac{b+a}{2}$$\n",
    "\n",
    "$$\\mathbb{E}\\left[Y^2\\right] = \\int_{a}^{b} y^2 \\frac{1}{b-a} \\,dy = \\frac{b^3-a^3}{3(b-a)} = \\frac{(b+a)^2-ab}{3}$$\n",
    "\n",
    "$$\\text{Var}\\left[Y\\right] = \\mathbb{E}\\left[Y^2\\right]-\\mathbb{E}\\left[Y\\right]^2 = \\frac{(b+a)^2-ab}{3} - \\frac{(b+a)^2}{4} = \\frac{(b-a)^2}{12}$$\n",
    "\n",
    "$$\\sigma_Y = \\sqrt{\\frac{(b-a)^2}{12}}$$\n",
    "\n",
    "For $X_1 \\sim Uniform(18,80)$:\n",
    "\n",
    "$$\\mathbb{E}\\left[X_1\\right] = \\frac{80+18}{2} = 49 \\text{,} \\quad \\text{Var}\\left[X_1\\right] =  \\frac{961}{3} = 320.\\bar{3} \\text{,} \\quad \\sigma_{X_1} = \\sqrt{961/3} \\approx  17.898 $$\n",
    "\n",
    "and for $X_2 \\sim Uniform(1,15)$:\n",
    "\n",
    "$$\\mathbb{E}\\left[X_2\\right] = \\frac{1+15}{2} = 8\\text{,} \\quad\\text{Var}\\left[X_2\\right] = \\frac{(15-1)^2}{12} = \\frac{49}{3} = 16.\\bar{3}, \\quad \\sigma_{X_2} = \\sqrt{49/3} \\approx  4.041 $$\n",
    "\n",
    "Suppose $B \\sim Bern(p)$. Then\n",
    "\n",
    "$$\\mathbb{E}\\left[B\\right] = \\sum_{k\\in S_B = \\{0,1\\}} b p^k (1-p)^{1-k} = p $$\n",
    "\n",
    "$$\\mathbb{E}\\left[B^2\\right] = \\sum_{k\\in \\{0,1\\}} b^2 p^k (1-p)^{1-k} = p $$\n",
    "\n",
    "$$\\text{Var}\\left[B\\right] = \\mathbb{E}\\left[B^2\\right]-\\mathbb{E}\\left[B\\right]^2 = p-p^2 = p(1-p)$$\n",
    "\n",
    "$$\\sigma_B = \\sqrt{p(1-p)}$$\n",
    "\n",
    "For $X_3\\sim Bern(0.1)$:\n",
    "\n",
    "$$\\mathbb{E}\\left[X_3\\right] = 0.1 \\text{,} \\quad \\text{Var}\\left[X_3\\right] = 0.09 \\text{,} \\quad \\sigma_{X_3} = 0.3 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Comparison: empirical and true statistics -----\n",
      "X1~Uniform((18, 80))\n",
      "\n",
      " Mean: \n",
      "\tTrue mean = 49.0, Empirical mean = 48.743\n",
      "\tDifference = 0.257 (0.525%)\n",
      "\n",
      " Standard deviation: \n",
      "\tTrue standard devitation = 17.898\n",
      "\t Population standard deviation:\n",
      "\t  Value = 18.007, Difference = 0.11 (0.612%)\n",
      "\t Sample standard deviation:\n",
      "\t  Value = 18.007, Difference = 0.11 (0.615%)\n",
      "-----------------------------------------------------\n",
      "\n",
      "----- Comparison: empirical and true statistics -----\n",
      "X2~Uniform((1, 15))\n",
      "\n",
      " Mean: \n",
      "\tTrue mean = 8.0, Empirical mean = 7.987\n",
      "\tDifference = 0.013 (0.168%)\n",
      "\n",
      " Standard deviation: \n",
      "\tTrue standard devitation = 4.041\n",
      "\t Population standard deviation:\n",
      "\t  Value = 4.031, Difference = 0.011 (0.263%)\n",
      "\t Sample standard deviation:\n",
      "\t  Value = 4.031, Difference = 0.011 (0.261%)\n",
      "-----------------------------------------------------\n",
      "\n",
      "----- Comparison: empirical and true statistics -----\n",
      "X3~Bernoulli(0.1)\n",
      "\n",
      " Mean: \n",
      "\tTrue mean = 0.1, Empirical mean = 0.102\n",
      "\tDifference = 0.002 (1.7%)\n",
      "\n",
      " Standard deviation: \n",
      "\tTrue standard devitation = 0.3\n",
      "\t Population standard deviation:\n",
      "\t  Value = 0.302, Difference = 0.002 (0.751%)\n",
      "\t Sample standard deviation:\n",
      "\t  Value = 0.302, Difference = 0.002 (0.754%)\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# a) calculate empirical means and standard deviations over training data\n",
    "\n",
    "def empirical_stat(x,dist=[]):\n",
    "    mean = x.mean()\n",
    "    s = x.std(ddof=1) # bias corrected df = 1 i.e. 1/(n-1)\n",
    "\n",
    "    if len(dist) == 3:\n",
    "        if dist[0] == \"Uniform\":\n",
    "            a,b = dist[1]\n",
    "            mu = (b+a) / 2\n",
    "            var = (b-a)**2 / 12\n",
    "            sigma = np.sqrt(var)\n",
    "        elif dist[0] == \"Bernoulli\":\n",
    "            p = dist[1]\n",
    "            mu = p\n",
    "            var = p*(1-p)\n",
    "            sigma = np.sqrt(var)\n",
    "        else:\n",
    "            return mean, s\n",
    "        rm = np.abs((mean-mu)/mu)*100\n",
    "        s_n = x.std() # by default df = 0 i.e. 1/n\n",
    "        \n",
    "        sr_n = np.abs((sigma-s_n)/sigma)*100\n",
    "        sr = np.abs((sigma-s)/sigma)*100\n",
    "\n",
    "\n",
    "        disp_str = \"\\n----- Comparison: empirical and true statistics -----\\n\"\n",
    "        disp_str += f\"{dist[2]}~{dist[0]}({dist[1]})\\n\\n\"\n",
    "        disp_str += f\" Mean: \\n\\tTrue mean = {round(mu,3)}, Empirical mean = {round(mean,3)}\"\n",
    "        disp_str += f\"\\n\\tDifference = {round(np.abs(mean-mu),3)} ({round(rm,3)}%)\"\n",
    "\n",
    "        disp_str += f\"\\n\\n Standard deviation: \\n\\tTrue standard devitation = {round(sigma,3)}\"\n",
    "        disp_str += f\"\\n\\t Population standard deviation:\"\n",
    "        disp_str += f\"\\n\\t  Value = {round(s_n,3)}\"\n",
    "        disp_str += f\", Difference = {round(np.abs(s_n-sigma),3)} ({round(sr_n,3)}%)\"\n",
    "\n",
    "        disp_str += f\"\\n\\t Sample standard deviation:\"\n",
    "        disp_str += f\"\\n\\t  Value = {round(s_n,3)}\"\n",
    "        disp_str += f\", Difference = {round(np.abs(s-sigma),3)} ({round(sr,3)}%)\"\n",
    "\n",
    "        disp_str += \"\\n\" + \"-\"*len(\"----- Comparison: empirical and true statistics -----\")\n",
    "\n",
    "        print(disp_str)\n",
    "   \n",
    "    return mean,s\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "## Using inbuilt functions\n",
    "x1_subs_mean, x1_subs_std = empirical_stat(x1_subs,[\"Uniform\",(a1,b1),\"X1\"])\n",
    "\n",
    "x2_subs_mean, x2_subs_std = empirical_stat(x2_subs,[\"Uniform\",(a2,b2),\"X2\"])\n",
    "\n",
    "x3_subs_mean, x3_subs_std = empirical_stat(x3_subs,[\"Bernoulli\",p,\"X3\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on the empirical standard devition**\n",
    "\n",
    "As can be seen above the sample and population standard deviations yield to almost identical results, but why then just stick with one and not the other?\n",
    "\n",
    "\n",
    "Suppose $\\mu$ and $\\sigma^2$ represent the true mean and variance respectfully, derived from some distribution $f_X$. If we were to estimate those measures purely based on observations. Then\n",
    "\n",
    "$$\\bar{X} = \\frac{1}{n}\\sum^n_{i=1} x_i \\text{,}\\quad\\hat{\\sigma}^2 = \\frac{1}{n} \\sum^n_{i=1}\\left(x_i-\\bar{X}\\right)^2$$\n",
    "\n",
    "Now if we take the expectations:\n",
    "\n",
    "$$\\mathbb{E}\\left[\\bar{X}\\right] = \\mathbb{E}\\left[\\frac{1}{n}\\sum^n_{i=1} x_i \\right] = \\frac{1}{n}\\sum^n_{i=1}\\mathbb{E}\\left[ x_i \\right] = \\mu$$\n",
    "\n",
    "and \n",
    "\n",
    "\n",
    "$$\\mathbb{E}\\left[\\hat{\\sigma}^2 \\right]= \\mathbb{E}\\left[\\frac{1}{n} \\sum^n_{i=1}\\left(x_i-\\bar{X}\\right)^2\\right] =  \\frac{1}{n}\\cdot\\mathbb{E}\\left[\\sum^n_{i=1}\\left(x_i-\\mu-(\\bar{X}-\\mu)\\right)^2\\right]= \\frac{1}{n}\\cdot\\mathbb{E}\\left[\\sum^n_{i=1}\\left((x_i-\\mu)^2+(\\bar{X}-\\mu)^2-2(\\bar{X}-\\mu)(x_i-\\mu)\\right)\\right]$$\n",
    "\n",
    "$$ = \\frac{1}{n}\\cdot\\mathbb{E}\\left[\\sum^n_{i=1}(x_i-\\mu)^2\\right]-\\frac{2}{n}\\cdot\\mathbb{E}\\left[(\\bar{X}-\\mu)\\sum^n_{i=1}\\left(x_i -\\mu\\right)\\right] + \\mathbb{E}\\left[(\\bar{X}-\\mu)^2\\right]\n",
    "= \\frac{1}{n}\\cdot\\mathbb{E}\\left[\\sum^n_{i=1}(x_i-\\mu)^2\\right] - \\mathbb{E}\\left[(\\bar{X}-\\mu)^2\\right] = \\sigma^2 - \\frac{\\sigma^2}{n} = \\frac{n-1}{n}\\sigma^2$$\n",
    "\n",
    "hence we need to correct for the bias of $n-1$ and define the unbiased estimator:\n",
    "\n",
    "$$\\tilde{\\sigma}^2 = \\frac{1}{n-1} \\sum^n_{i=1}\\left(x_i-\\bar{X}\\right)^2$$\n",
    "\n",
    "which, is in fact the definition of the sample variance. Taking the square root gives the sample standard deviation which is a still biased estimator for $\\sigma$ but Bessel-correct, which we will consider to be sufficient (as long as n is relatively large).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Can you come up with a few (2 or 3) additional features that may be relevant?\n",
    "# (you don't have to implement those of course, just write down your answer in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.b**\n",
    "\n",
    "Based on intution and past experiance we suggest the following:\n",
    "\n",
    "- Loan amount\n",
    "\n",
    "    This should be very relevant, especially in contrast to the salary. \n",
    "    Suppose the two extremes:\n",
    "    \n",
    "    Individual A is 50 years old, has a salary of 13'000 CHF a month and is obligated to pay off a loan of 10'000 CHF, here it should be rather obvious that the PD (probability of default) should be zero, hence the exposure is neglible. Especially if the payment is not due until much later or spread over a long period.\n",
    "\n",
    "    Individual B is 50 years old, has a salary of 3'000 CHF a month and is obligated to pay off a loan of 500'000 CHF, here it doesn't seem as obvious how the individual is supposed to pay off the loan. If we assume that the individual has no other financial obligations and pays no taxes then it would still take him almost 14 years to simply gather the principal amount. If we consider interest rates, which accumulated it would be even harder for the individual to pay off the loan.\n",
    "\n",
    "- Loan term\n",
    "    \n",
    "    This is somewhat self-explanatory, the nature of the loan is what should matter. Take individual B again, what if he is obligated to pay off the loan in 30 years compared to only 10. The PD will by effected by this feature, hence it should be included.\n",
    "\n",
    "- Interest rates\n",
    "\n",
    "    Again, this is somewhat self-explanatory, the cost of the loan matters. Take a new individual C, suppose he is 39 years old, has a salary of 5'000 CHF a month and is obligated to pay off a loan of 200'000 CHF over a period of 20 years, seems reasonable. But, what if bearing yearly compounded interest rates were 18%? the total amount that will be paid in the end is astronomically higher the principal, whereas the accumulated amount wont multiply as often for slightly lower interest rates. Hence, this feature should be considered to be relevant.\n",
    "\n",
    "- Credit history\n",
    "\n",
    "    If we had access to the individuals' default history it should be obvious that the relationship between defaulting once or more should be strongly linked to the probability defaulting again. One could argue that an individual with a long history of meeting obligations should be compensated. This could potentially be a feature with two subfeatures of more, for instance having the number of defaults, the number months of active obligations, number of such obligations etc.\n",
    "\n",
    "- Loan type\n",
    "\n",
    "    The purpose would be to distinguish between overdraft, mortgage, student loans, auto loans etc. This could already be somewhat reflected in the aforementioned features, yet one could consider the following counterexample, suppose there are two individuals with identical (risk) profiles and further assume all other features are equal, if one was using the borrowed amount to finance a house while the other was using the amount to purchase very volatile cryptocurrency, it is obvious the invidual should not get equal PD values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhR3TshATJOo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 2.\n",
    "# Building the datasets:\n",
    "\n",
    "sigmoid = lambda x: 1. / (1. + np.exp(-x))\n",
    "\n",
    "eps = np.random.uniform(0,1)\n",
    "\n",
    "p1 = lambda x1,x2,x3 : sigmoid(13.3-0.33*x1+3.5*x2-3*x3)\n",
    "p2 = lambda x1,x2,x3 : sigmoid(5-10*((x1<25)+(x1>75))+1.1*x2-x3)\n",
    "\n",
    "y1 = lambda x1,x2,x3 : (eps<=p1(x1,x2,x3))\n",
    "y2 = lambda x1,x2,x3 : (eps<=p2(x1,x2,x3))\n",
    "\n",
    "\n",
    "# build the first dataset\n",
    "\n",
    "\n",
    "# build the second dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Rz30ywUHUOSG"
   },
   "outputs": [],
   "source": [
    "# Exercise 2. a)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "# \"model = LogisticRegression().fit(X_data, Y_data)\" fits a model\n",
    "# \"pred_X = model.predict_proba(X)\" evaluates the model\n",
    "# (note that it outputs both P(Y=0|X) and P(Y=1|X))\n",
    "# \"log_loss(Y, pred_X)\" evaluates the negative conditional log likelihood (also called cross-entropy loss)\n",
    "\n",
    "# Fit the models on both datasets\n",
    "\n",
    "\n",
    "# Calculate cross-entropy loss on both datasets for train and test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZJawLeRKWIQJ"
   },
   "outputs": [],
   "source": [
    "# Exercise 2. b)\n",
    "# Calculate normalized data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QaQx76yCWOa3"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.b) (i) and (ii)\n",
    "from sklearn.svm import SVC\n",
    "# \"model = SVC(kernel='rbf', gamma=GAMMA, C=C, probability=True)\" creates\n",
    "# a model with kernel exp(-GAMMA \\|x-x'\\|_2^2) and regul. parameter C (note the relation between C and the parameter lambda from the lecture).\n",
    "# \"probability=True\" enables the option \"model.predict_proba(X)\" to predict probabilities from the regression function \\hat{f}^{svm}.\n",
    "# \"model.fit(X, Y)\" optimizes the model parameters (using hinge loss)\n",
    "\n",
    "# Fit the models for both datasets (this can take up to 60 seconds with SVC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mS2tjfLdYO4Z"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.b (iii)\n",
    "# \"model.predict_proba(X)\" predicts probabilities from features (note that it outputs both P(Y=0|X) and P(Y=1|X))\n",
    "\n",
    "# Calculate cross-entropy loss on both normalized datasets for train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.b (iv)\n",
    "#Would the results change using standardized data instead of normalized data? No need to run any code here, only explain your reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7xTDD8SNZhZz"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.c\n",
    "import matplotlib.pyplot as plt\n",
    "# To calculate the curves, it is fine to take 100 threshold values c, i.e.,\n",
    "ths = np.linspace(0, 1, 100)\n",
    "\n",
    "# To approximately calculate the AUC, it is fine to simply use Riemann sums.\n",
    "# This means, if you have 100 (a_i, b_i) pairs for the curves, a_1 <= a_2 <= ...\n",
    "# then you may simply use the sum\n",
    "# sum_{i=1}^99 (b_i + b_{i+1})/2 * (a_{i+1}-a_i)\n",
    "# as the approximation of the integral (or AUC)\n",
    "\n",
    "\n",
    "# first data set & logistic regression:\n",
    "# (the code should be reusable for all cases, only exchanging datasets and predicted probabilities depending on the model)\n",
    "\n",
    "# Calculate positives (only depending on the dataset)\n",
    "\n",
    "# Calculate true positives for all threshold values\n",
    "\n",
    "# Calculate false positives for all threshold values\n",
    "\n",
    "# Calculate FDR and TPR rate (points on the FDR/TPR curve) and the AUC\n",
    "\n",
    "\n",
    "# second data set & logistic regression:\n",
    "\n",
    "\n",
    "# first data set and rkhs regression:\n",
    "\n",
    "\n",
    "# second data set and rkhs regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5YvGSeDoc9ZS"
   },
   "outputs": [],
   "source": [
    "# Exercise 3.\n",
    "\n",
    "# Set model parameters and define matrix D\n",
    "\n",
    "\n",
    "# Scenario 1:\n",
    "# Define Portfolio and possible outcomes for this portfolio using matrix D\n",
    "\n",
    "\n",
    "# Plot histogram of profits and losses\n",
    "\n",
    "\n",
    "# Calculate expected profit and losses and 95%-VaR\n",
    "\n",
    "\n",
    "# Scenario 2:\n",
    "# Define Portfolio and possible outcomes using the matrix D and the predicted default probabilities from the logistic regression model\n",
    "\n",
    "\n",
    "# Plot histogram of profits and losses\n",
    "\n",
    "\n",
    "# Calculate expected profit and losses and 95%-VaR\n",
    "\n",
    "\n",
    "# Scenario 3:\n",
    "# Define Portfolio and possible outcomes using the matrix D and the predicted default probabilities from the rkhs model\n",
    "\n",
    "\n",
    "# Plot histogram of profits & losses (which is simply the performance of each strategy in scenario k, i.e. a profit if positive or a loss if negative)\n",
    "\n",
    "\n",
    "# Calculate expected profit & losses and 95%-VaR for each strategy"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
