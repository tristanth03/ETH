{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVNblDBpVoAq"
   },
   "source": [
    "**Names of all group members:**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "All code below is only suggestive and you may as well use different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "id": "K1qPukjnSRgc"
   },
   "outputs": [],
   "source": [
    "# Exercise 1.\n",
    "import numpy as np\n",
    "np.random.seed(0)  # for reproducibility\n",
    "\n",
    "# simulate explanatory variables x (the age and monthly income are assumed here to follow a continous uniform distribution)\n",
    "m = 20_000\n",
    "n = 10_000\n",
    "\n",
    "a1,b1 = 18,80\n",
    "x1 = np.random.uniform(a1,b1,m+n) # age\n",
    "\n",
    "a2,b2 = 1,15\n",
    "x2 = np.random.uniform(a2,b2,m+n) # monthly income in 1'000 CHF\n",
    "\n",
    "p = 0.1\n",
    "x3 = np.random.binomial(1,p,m+n) # salaried/self-employed with x3~bern(p) with p = 0.1\n",
    "\n",
    "x1_subs = x1[0:m]\n",
    "x2_subs = x2[0:m]\n",
    "x3_subs = x3[0:m]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly derive the first two moments for comparison purposes.\n",
    "\n",
    "Suppose $Y \\sim Uniform(a,b)$. Then\n",
    "\n",
    "$$\\mathbb{E}\\left[Y\\right] = \\int_{a}^{b} y \\frac{1}{b-a} \\,dy = \\frac{b^2-a^2}{2(b-a)} = \\frac{b+a}{2}$$\n",
    "\n",
    "$$\\mathbb{E}\\left[Y^2\\right] = \\int_{a}^{b} y^2 \\frac{1}{b-a} \\,dy = \\frac{b^3-a^3}{3(b-a)} = \\frac{(b+a)^2-ab}{3}$$\n",
    "\n",
    "$$\\text{Var}\\left[Y\\right] = \\mathbb{E}\\left[Y^2\\right]-\\mathbb{E}\\left[Y\\right]^2 = \\frac{(b+a)^2-ab}{3} - \\frac{(b+a)^2}{4} = \\frac{(b-a)^2}{12}$$\n",
    "\n",
    "$$\\sigma_Y = \\sqrt{\\frac{(b-a)^2}{12}}$$\n",
    "\n",
    "For $X_1 \\sim Uniform(18,80)$:\n",
    "\n",
    "$$\\mathbb{E}\\left[X_1\\right] = \\frac{80+18}{2} = 49 \\text{,} \\quad \\text{Var}\\left[X_1\\right] =  \\frac{961}{3} = 320.\\bar{3} \\text{,} \\quad \\sigma_{X_1} = \\sqrt{961/3} \\approx  17.898 $$\n",
    "\n",
    "and for $X_2 \\sim Uniform(1,15)$:\n",
    "\n",
    "$$\\mathbb{E}\\left[X_2\\right] = \\frac{1+15}{2} = 8\\text{,} \\quad\\text{Var}\\left[X_2\\right] = \\frac{(15-1)^2}{12} = \\frac{49}{3} = 16.\\bar{3}, \\quad \\sigma_{X_2} = \\sqrt{49/3} \\approx  4.041 $$\n",
    "\n",
    "Suppose $B \\sim Bern(p)$. Then\n",
    "\n",
    "$$\\mathbb{E}\\left[B\\right] = \\sum_{k\\in S_B = \\{0,1\\}} b p^k (1-p)^{1-k} = p $$\n",
    "\n",
    "$$\\mathbb{E}\\left[B^2\\right] = \\sum_{k\\in \\{0,1\\}} b^2 p^k (1-p)^{1-k} = p $$\n",
    "\n",
    "$$\\text{Var}\\left[B\\right] = \\mathbb{E}\\left[B^2\\right]-\\mathbb{E}\\left[B\\right]^2 = p-p^2 = p(1-p)$$\n",
    "\n",
    "$$\\sigma_B = \\sqrt{p(1-p)}$$\n",
    "\n",
    "For $X_3\\sim Bern(0.1)$:\n",
    "\n",
    "$$\\mathbb{E}\\left[X_3\\right] = 0.1 \\text{,} \\quad \\text{Var}\\left[X_3\\right] = 0.09 \\text{,} \\quad \\sigma_{X_3} = 0.3 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Comparison: empirical and true statistics -----\n",
      "X1~Uniform((18, 80))\n",
      "\n",
      " Mean: \n",
      "\tTrue mean = 49.0, Empirical mean = 48.743\n",
      "\tDifference = 0.257 (0.525%)\n",
      "\n",
      " Standard deviation: \n",
      "\tTrue standard devitation = 17.898\n",
      "\t Population standard deviation:\n",
      "\t  Value = 18.007, Difference = 0.11 (0.612%)\n",
      "\t Sample standard deviation:\n",
      "\t  Value = 18.007, Difference = 0.11 (0.615%)\n",
      "-----------------------------------------------------\n",
      "\n",
      "----- Comparison: empirical and true statistics -----\n",
      "X2~Uniform((1, 15))\n",
      "\n",
      " Mean: \n",
      "\tTrue mean = 8.0, Empirical mean = 7.987\n",
      "\tDifference = 0.013 (0.168%)\n",
      "\n",
      " Standard deviation: \n",
      "\tTrue standard devitation = 4.041\n",
      "\t Population standard deviation:\n",
      "\t  Value = 4.031, Difference = 0.011 (0.263%)\n",
      "\t Sample standard deviation:\n",
      "\t  Value = 4.031, Difference = 0.011 (0.261%)\n",
      "-----------------------------------------------------\n",
      "\n",
      "----- Comparison: empirical and true statistics -----\n",
      "X3~Bernoulli(0.1)\n",
      "\n",
      " Mean: \n",
      "\tTrue mean = 0.1, Empirical mean = 0.102\n",
      "\tDifference = 0.002 (1.7%)\n",
      "\n",
      " Standard deviation: \n",
      "\tTrue standard devitation = 0.3\n",
      "\t Population standard deviation:\n",
      "\t  Value = 0.302, Difference = 0.002 (0.751%)\n",
      "\t Sample standard deviation:\n",
      "\t  Value = 0.302, Difference = 0.002 (0.754%)\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# a) calculate empirical means and standard deviations over training data\n",
    "\n",
    "def empirical_stat(x,dist=[]):\n",
    "    mean = x.mean()\n",
    "    s = x.std(ddof=1) # bias corrected df = 1 i.e. 1/(n-1)\n",
    "\n",
    "    if len(dist) == 3:\n",
    "        if dist[0] == \"Uniform\":\n",
    "            a,b = dist[1]\n",
    "            mu = (b+a) / 2\n",
    "            var = (b-a)**2 / 12\n",
    "            sigma = np.sqrt(var)\n",
    "        elif dist[0] == \"Bernoulli\":\n",
    "            p = dist[1]\n",
    "            mu = p\n",
    "            var = p*(1-p)\n",
    "            sigma = np.sqrt(var)\n",
    "        else:\n",
    "            return mean, s\n",
    "        rm = np.abs((mean-mu)/mu)*100\n",
    "        s_n = x.std() # by default df = 0 i.e. 1/n\n",
    "        \n",
    "        sr_n = np.abs((sigma-s_n)/sigma)*100\n",
    "        sr = np.abs((sigma-s)/sigma)*100\n",
    "\n",
    "\n",
    "        disp_str = \"\\n----- Comparison: empirical and true statistics -----\\n\"\n",
    "        disp_str += f\"{dist[2]}~{dist[0]}({dist[1]})\\n\\n\"\n",
    "        disp_str += f\" Mean: \\n\\tTrue mean = {round(mu,3)}, Empirical mean = {round(mean,3)}\"\n",
    "        disp_str += f\"\\n\\tDifference = {round(np.abs(mean-mu),3)} ({round(rm,3)}%)\"\n",
    "\n",
    "        disp_str += f\"\\n\\n Standard deviation: \\n\\tTrue standard devitation = {round(sigma,3)}\"\n",
    "        disp_str += f\"\\n\\t Population standard deviation:\"\n",
    "        disp_str += f\"\\n\\t  Value = {round(s_n,3)}\"\n",
    "        disp_str += f\", Difference = {round(np.abs(s_n-sigma),3)} ({round(sr_n,3)}%)\"\n",
    "\n",
    "        disp_str += f\"\\n\\t Sample standard deviation:\"\n",
    "        disp_str += f\"\\n\\t  Value = {round(s_n,3)}\"\n",
    "        disp_str += f\", Difference = {round(np.abs(s-sigma),3)} ({round(sr,3)}%)\"\n",
    "\n",
    "        disp_str += \"\\n\" + \"-\"*len(\"----- Comparison: empirical and true statistics -----\")\n",
    "\n",
    "        print(disp_str)\n",
    "   \n",
    "    return mean,s\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "## Using inbuilt functions\n",
    "x1_subs_mean, x1_subs_std = empirical_stat(x1_subs,[\"Uniform\",(a1,b1),\"X1\"])\n",
    "\n",
    "x2_subs_mean, x2_subs_std = empirical_stat(x2_subs,[\"Uniform\",(a2,b2),\"X2\"])\n",
    "\n",
    "x3_subs_mean, x3_subs_std = empirical_stat(x3_subs,[\"Bernoulli\",p,\"X3\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on the empirical standard devition**\n",
    "\n",
    "As can be seen above the sample and population standard deviations yield to almost identical results, but why then just stick with one and not the other?\n",
    "\n",
    "\n",
    "Suppose $\\mu$ and $\\sigma^2$ represent the true mean and variance respectfully, derived from some distribution $f_X$. If we were to estimate those measures purely based on observations. Then\n",
    "\n",
    "$$\\bar{X} = \\frac{1}{n}\\sum^n_{i=1} x_i \\text{,}\\quad\\hat{\\sigma}^2 = \\frac{1}{n} \\sum^n_{i=1}\\left(x_i-\\bar{X}\\right)^2$$\n",
    "\n",
    "Now if we take the expectations:\n",
    "\n",
    "$$\\mathbb{E}\\left[\\bar{X}\\right] = \\mathbb{E}\\left[\\frac{1}{n}\\sum^n_{i=1} x_i \\right] = \\frac{1}{n}\\sum^n_{i=1}\\mathbb{E}\\left[ x_i \\right] = \\mu$$\n",
    "\n",
    "and \n",
    "\n",
    "\n",
    "$$\\mathbb{E}\\left[\\hat{\\sigma}^2 \\right]= \\mathbb{E}\\left[\\frac{1}{n} \\sum^n_{i=1}\\left(x_i-\\bar{X}\\right)^2\\right] =  \\frac{1}{n}\\cdot\\mathbb{E}\\left[\\sum^n_{i=1}\\left(x_i-\\mu-(\\bar{X}-\\mu)\\right)^2\\right]= \\frac{1}{n}\\cdot\\mathbb{E}\\left[\\sum^n_{i=1}\\left((x_i-\\mu)^2+(\\bar{X}-\\mu)^2-2(\\bar{X}-\\mu)(x_i-\\mu)\\right)\\right]$$\n",
    "\n",
    "$$ = \\frac{1}{n}\\cdot\\mathbb{E}\\left[\\sum^n_{i=1}(x_i-\\mu)^2\\right]-\\frac{2}{n}\\cdot\\mathbb{E}\\left[(\\bar{X}-\\mu)\\sum^n_{i=1}\\left(x_i -\\mu\\right)\\right] + \\mathbb{E}\\left[(\\bar{X}-\\mu)^2\\right]\n",
    "= \\frac{1}{n}\\cdot\\mathbb{E}\\left[\\sum^n_{i=1}(x_i-\\mu)^2\\right] - \\mathbb{E}\\left[(\\bar{X}-\\mu)^2\\right] = \\sigma^2 - \\frac{\\sigma^2}{n} = \\frac{n-1}{n}\\sigma^2$$\n",
    "\n",
    "hence we need to correct for the bias of $n-1$ and define the unbiased estimator:\n",
    "\n",
    "$$\\tilde{\\sigma}^2 = \\frac{1}{n-1} \\sum^n_{i=1}\\left(x_i-\\bar{X}\\right)^2$$\n",
    "\n",
    "which, is in fact the definition of the sample variance. Taking the square root gives the sample standard deviation which is a still biased estimator for $\\sigma$ but Bessel-correct, which we will consider to be sufficient (as long as n is relatively large).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Can you come up with a few (2 or 3) additional features that may be relevant?\n",
    "# (you don't have to implement those of course, just write down your answer in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.b\n",
    "\n",
    "Based on intution and past experiance we suggest the following:\n",
    "\n",
    "- Loan amount\n",
    "\n",
    "    This should be very relevant, especially in contrast to the salary. \n",
    "    Suppose the two extremes:\n",
    "    \n",
    "    Individual A is 50 years old, has a salary of 13'000 CHF a month and is obligated to pay off a loan of 10'000 CHF, here it should be rather obvious that the PD (probability of default) should be $\\approx 0$, hence the exposure is neglible. Especially if the payment is not due until much later or spread over a long period.\n",
    "\n",
    "    Individual B is 50 years old, has a salary of 3'000 CHF a month and is obligated to pay off a loan of 500'000 CHF, here it does not seem as obvious how the individual is supposed to pay off the loan. If we assume that the individual has no other financial obligations, pays no taxes and the cost of the loan is zero then it would still take him almost 14 years to simply gather the principal amount. If we consider interest rates, which would accumulated, it would be even harder for the individual to pay off the loan.\n",
    "\n",
    "- Loan term\n",
    "    \n",
    "    This is somewhat self-explanatory, the nature of the loan is what should matter. Take individual B again, what if he is obligated to pay off the loan in 30 years compared to only 10. The PD will by effected by this feature, hence it should be included.\n",
    "\n",
    "- Interest rates\n",
    "\n",
    "    Again, this is somewhat self-explanatory, the cost of the loan matters. Take a new individual C, suppose he is 39 years old, has a salary of 5'000 CHF a month and is obligated to pay off a loan of 200'000 CHF over a period of 20 years, seems reasonable. But, what if bearing yearly compounded interest rates were 18%? the total amount that will be paid in the end is astronomically higher the principal, whereas the accumulated amount will not multiply as often for slightly lower interest rates. Hence, this feature should be considered to be relevant.\n",
    "\n",
    "- Credit history\n",
    "\n",
    "    If we had access to the individuals' default history it should seem rather obvious that the relationship between defaulting once or more should be strongly linked to the probability of defaulting again. One could argue that an individual with a long history of meeting obligations should be compensated. This could potentially be a feature with two subfeatures or more. For instance, tracking the number of defaults, the number months of active obligations, number of such obligations etc.\n",
    "\n",
    "- Loan type\n",
    "\n",
    "    The purpose would be to distinguish between overdraft, mortgage, student loans, auto loans etc. This could already be somewhat reflected in the aforementioned features, yet one could argue against it. Suppose there are two individuals with identical (risk) profiles and further assume all other features are equal, if one was using the borrowed amount to finance a house while the other was using the amount to purchase very volatile cryptocurrency, it is obvious the inviduals should not get equal PD values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "id": "RhR3TshATJOo"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.\n",
    "# Building the datasets:\n",
    "\n",
    "sigmoid = lambda x: 1. / (1. + np.exp(-x))\n",
    "\n",
    "eps = np.random.uniform(0,1,m+n)\n",
    "\n",
    "p1 = lambda x1,x2,x3 : sigmoid(13.3-0.33*x1+3.5*x2-3*x3)\n",
    "p2 = lambda x1,x2,x3 : sigmoid(5-10*((x1<25)+(x1>75))+1.1*x2-x3)\n",
    "\n",
    "# build the first dataset\n",
    "y1 = (eps<=p1(x1,x2,x3)).astype(int)\n",
    "\n",
    "# build the second dataset\n",
    "y2 = (eps<=p2(x1,x2,x3)).astype(int)\n",
    "\n",
    "y1_train,y1_test = y1[0:m],y1[m:n+m]\n",
    "\n",
    "y2_train,y2_test = y2[0:m],y2[m:n+m]\n",
    "x = np.stack((x1,x2,x3),axis=1) # fit takes (n+m,d)\n",
    "x_train,x_test = x[0:m],x[m:n+m]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "id": "Rz30ywUHUOSG"
   },
   "outputs": [],
   "source": [
    "# Exercise 2. a)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "model1 = LogisticRegression().fit(x_train, y1_train)\n",
    "model2 = LogisticRegression().fit(x_train, y2_train)\n",
    "\n",
    "theta = 0.5\n",
    "def class_assign(y_p,threshold=theta):\n",
    "    return (y_p[:,1]>threshold).astype(int)\n",
    "\n",
    "y1_hat_train_p,y1_hat_test_p = model1.predict_proba(x_train),model1.predict_proba(x_test)\n",
    "y2_hat_train_p,y2_hat_test_p = model2.predict_proba(x_train),model2.predict_proba(x_test)\n",
    "\n",
    "y1_hat_train, y1_hat_test = class_assign(y1_hat_train_p),class_assign(y1_hat_test_p)\n",
    "y2_hat_train, y2_hat_test = class_assign(y2_hat_train_p),class_assign(y2_hat_test_p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on binary cross-entropy loss**\n",
    "\n",
    "The binary cross-entropy or log loss is defined as:\n",
    "$$\\mathcal{L}(y;p) = -\\frac{1}{n}\\sum_{i=1}^n (y_i\\ln(p_i)+(1-y_i)\\ln(1-p_i))$$\n",
    "\n",
    "now if we define the probability of assigned to the true class\n",
    "\n",
    "$$p_i^{true}= \\begin{cases}p_i, &  y_i = 1 \\\\ 1-p_i, &  y_i = 0\\end{cases}$$\n",
    "\n",
    "then \n",
    "\n",
    "$$ -\\frac{1}{n}\\sum_{i=1}^n (y_i\\ln(p_i)+(1-y_i)\\ln(1-p_i)) = -\\frac{1}{n}\\sum_{i=1}^n \\ln(p_i^{true})$$\n",
    "\n",
    "and take the negative exponential\n",
    "\n",
    "$$\\exp(-\\mathcal{L}(y;p)) = \\exp\\left(\\frac{1}{n}\\sum_{i=1}^n \\ln(p_i^{true})\\right) = \\left(\\prod_{i=1}^n p_i^{true}\\right)^{1/n} $$\n",
    "\n",
    "then we have the geometric mean of the probabilities that model assigned a true label to the data, so roughly speaking it represents the confidence the model has in assigning to the true class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Comparison: train and test loss -----\n",
      "p1:\n",
      "  Loss:\n",
      "\ttrain = 2.965e-02, test = 3.486e-02\n",
      "  Confidence:\n",
      "\ttrain = 0.971, test = 0.966\n",
      "  Class Balance (at threshold=0.5):\n",
      "\t train:\t  True: (0.9515, 0.0485), Prediction: (0.95295, 0.04705)\n",
      "\t test:\t  True: (0.9523, 0.0477), Prediction: (0.9549, 0.0451)\n",
      "p2:\n",
      "  Loss:\n",
      "\ttrain = 1.535e-01, test = 1.486e-01\n",
      "  Confidence:\n",
      "\ttrain = 0.858, test = 0.862\n",
      "  Class Balance (at threshold=0.5):\n",
      "\t train:\t  True: (0.9482, 0.0518), Prediction: (1.0, 0.0)\n",
      "\t test:\t  True: (0.9513, 0.0487), Prediction: (1.0, 0.0)\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Class split counts\n",
    "def splits(y):\n",
    "    good, bad = sum((y==1)), sum((y==0))\n",
    "    c_g = good / (good+bad)\n",
    "    c_b = bad / (good+bad)\n",
    "    return c_g,c_b\n",
    "\n",
    "\n",
    "def comparison(y1_train,y1_test,y2_train,y2_test,\n",
    "               y1_hat_train,y1_hat_test,y2_hat_train,y2_hat_test,\n",
    "               y1_hat_train_p,y1_hat_test_p,y2_hat_train_p,y2_hat_test_p,\n",
    "               show=True):\n",
    "    \n",
    "\n",
    "    loss1_train,loss1_test = log_loss(y_true=y1_train,y_pred=y1_hat_train_p),log_loss(y_true=y1_test,y_pred=y1_hat_test_p)\n",
    "    loss2_train,loss2_test = log_loss(y_true=y2_train,y_pred=y2_hat_train_p),log_loss(y_true=y2_test,y_pred=y2_hat_test_p)\n",
    "\n",
    "    if show:\n",
    "        c1_train,c1_test = splits(y1_train),splits(y1_test)\n",
    "        c2_train,c2_test = splits(y2_train),splits(y2_test)\n",
    "        c1_hat_train,c1_hat_test = splits(y1_hat_train),splits(y1_hat_test)\n",
    "        c2_hat_train,c2_hat_test = splits(y2_hat_train),splits(y2_hat_test)\n",
    "\n",
    "        loss_str = \"\\n----- Comparison: train and test loss -----\\n\"\n",
    "        loss_str += \"p1:\\n\"\n",
    "        loss_str += \"  Loss:\\n\"\n",
    "        loss_str += f\"\\ttrain = {loss1_train:.3e}, test = {loss1_test:.3e}\\n\"\n",
    "        loss_str += \"  Confidence:\\n\"\n",
    "        loss_str += f\"\\ttrain = {np.exp(-loss1_train):.3f}, test = {np.exp(-loss1_test):.3f}\\n\"\n",
    "        loss_str += f\"  Class Balance (at threshold={theta}):\\n\"\n",
    "        loss_str += f\"\\t train:\" \n",
    "        loss_str += f\"\\t  True: {c1_train}, Prediction: {c1_hat_train}\\n\" \n",
    "        loss_str += f\"\\t test:\" \n",
    "        loss_str += f\"\\t  True: {c1_test}, Prediction: {c1_hat_test}\\n\" \n",
    "\n",
    "        loss_str += \"p2:\\n\"\n",
    "        loss_str += \"  Loss:\\n\"\n",
    "        loss_str += f\"\\ttrain = {loss2_train:.3e}, test = {loss2_test:.3e}\\n\"\n",
    "        loss_str += \"  Confidence:\\n\"\n",
    "        loss_str += f\"\\ttrain = {np.exp(-loss2_train):.3f}, test = {np.exp(-loss2_test):.3f}\\n\"\n",
    "        loss_str += f\"  Class Balance (at threshold={theta}):\\n\"\n",
    "        loss_str += f\"\\t train:\" \n",
    "        loss_str += f\"\\t  True: {c2_train}, Prediction: {c2_hat_train}\\n\" \n",
    "        loss_str += f\"\\t test:\" \n",
    "        loss_str += f\"\\t  True: {c2_test}, Prediction: {c2_hat_test}\" \n",
    "        loss_str += \"\\n\" + \"-\"*len(\"----- Comparison: train and test loss -----\")\n",
    "\n",
    "        print(loss_str)\n",
    "    return loss1_train,loss1_test,loss2_train,loss2_test\n",
    "\n",
    "loss1_train,loss1_test,loss2_train,loss2_test = comparison(y1_train,y1_test,y2_train,y2_test,\n",
    "               y1_hat_train,y1_hat_test,y2_hat_train,y2_hat_test,\n",
    "               y1_hat_train_p,y1_hat_test_p,y2_hat_train_p,y2_hat_test_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on the results (2.a)**\n",
    "\n",
    "For dataset $s=1$ the loss and thus the confidence is reasonable, the model very clearly distinguishes between $y_i = 0$ and $y_i = 1$. This applies to both the training and the test loss, note the test loss is slightly higher, hence there might be marginal overfit (so small that it is neglible). Furthermore the (im)balance between classes is quite similar between the truth and the prediction, at the threshold decider of $\\theta = 0.5$ (split in the middle). In short, it performs and generalizes rather well. It could serve reasonably well as a starting point.\n",
    "\n",
    "In the case of dataset $s=2$ one might mistakenly take the confidence of $\\approx 0.86$ as fine, (it is not!) since the classes are highly imbalanced, furthermore the loss is not great (in comparison to $s=1$). But, the results are \"bad\" across the training and test set, hence it generalizes. Just to point it out at this this threshold the model simply does one thing, picks the more frequent class. Which is far from what we want, even after some threshold optimization one should not use this model as it is (but more on that later). \n",
    "\n",
    "Suggestion:\n",
    "- In both cases the imbalance will need to be treated specially\n",
    "\n",
    "- In both cases we should to consider to standardize/scale/normalize the feature variables\n",
    "\n",
    "- In $s=2$ we should strongly consider a more complex solution (note that even if we did not know that the underlying function which determines $y_2$ is less trivial than the one that determines $y_1$ we should still consider/recommend a more sophisticated approach)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "id": "ZJawLeRKWIQJ"
   },
   "outputs": [],
   "source": [
    "# Exercise 2. b)\n",
    "# Calculate normalized data\n",
    "x_tilde = np.stack((x1 / x1_subs_std,x2 / x2_subs_std,x3 / x3_subs_std),axis=1)\n",
    "x_tilde_train, x_tilde_test = x_tilde[0:m],x_tilde[m:n+m]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "id": "QaQx76yCWOa3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=0.2, gamma=0.1, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=0.2, gamma=0.1, probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=0.2, gamma=0.1, probability=True)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 2.b) (i) and (ii)\n",
    "from sklearn.svm import SVC\n",
    "C = 1/5\n",
    "gamma = 1/10\n",
    "model_s1 = SVC(kernel='rbf', gamma=gamma, C=C, probability=True)\n",
    "model_s2 = SVC(kernel='rbf', gamma=gamma, C=C, probability=True)\n",
    "\n",
    "#  creates a model with kernel exp(-GAMMA \\|x-x'\\|_2^2) and regul. parameter C (note the relation between C and the parameter lambda from the lecture).\n",
    "# \"probability=True\" enables the option \"model.predict_proba(X)\" to predict probabilities from the regression function \\hat{f}^{svm}.\n",
    "# \"model.fit(X, Y)\" optimizes the model parameters (using hinge loss)\n",
    "# Fit the models for both datasets (this can take up to 60 seconds with SVC)\n",
    "model_s1.fit(x_tilde_train,y1_train)\n",
    "model_s2.fit(x_tilde_train,y2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.b (ii)**\n",
    "\n",
    "In order obtain the calibrated probabilities $\\hat{p}_s^{\\text{svm}} = \\hat{g}_s \\circ \\hat{f}^{\\text{svm}}_s$  we fit logistic function\n",
    "\n",
    "$$\\hat{g}(z)=\\frac{1}{1+\\exp(\\alpha z +\\beta)},$$\n",
    "\n",
    "where $\\alpha,\\beta$ are found by solving the following logistic regression problem:\n",
    "\n",
    "$$\\min_{\\alpha,\\beta} \\sum \\left( \\{-y_i\\ln(\\hat{g}(\\hat{f}_i))\\} + \\{(1-y_i)\\ln(1-\\hat{g}(\\hat{f}_i))\\} \\right)  \\leq 0$$\n",
    "\n",
    "The solution to this corresponds to the maximum likelihood estimation of the sigmoid parameters, which ensure the SVM calibration. This is already included in the ``SVC`` class, therefore in practice we turn on the parameter ``probability=True``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "id": "mS2tjfLdYO4Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Comparison: train and test loss -----\n",
      "p1:\n",
      "  Loss:\n",
      "\ttrain = 3.191e-02, test = 3.878e-02\n",
      "  Confidence:\n",
      "\ttrain = 0.969, test = 0.962\n",
      "  Class Balance (at threshold=0.5):\n",
      "\t train:\t  True: (0.9515, 0.0485), Prediction: (0.95225, 0.04775)\n",
      "\t test:\t  True: (0.9523, 0.0477), Prediction: (0.9543, 0.0457)\n",
      "p2:\n",
      "  Loss:\n",
      "\ttrain = 7.109e-02, test = 6.707e-02\n",
      "  Confidence:\n",
      "\ttrain = 0.931, test = 0.935\n",
      "  Class Balance (at threshold=0.5):\n",
      "\t train:\t  True: (0.9482, 0.0518), Prediction: (0.953, 0.047)\n",
      "\t test:\t  True: (0.9513, 0.0487), Prediction: (0.9549, 0.0451)\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.b (iii)\n",
    "# \"model.predict_proba(X)\" predicts probabilities from features (note that it outputs both P(Y=0|X) and P(Y=1|X))\n",
    "y1_hat_train_p_s,y1_hat_test_p_s = model_s1.predict_proba(x_tilde_train),model_s1.predict_proba(x_tilde_test)\n",
    "y2_hat_train_p_s,y2_hat_test_p_s = model_s2.predict_proba(x_tilde_train),model_s2.predict_proba(x_tilde_test)\n",
    "\n",
    "y1_hat_train_s, y1_hat_test_s = class_assign(y1_hat_train_p_s),class_assign(y1_hat_test_p_s)\n",
    "y2_hat_train_s, y2_hat_test_s = class_assign(y2_hat_train_p_s),class_assign(y2_hat_test_p_s)\n",
    "\n",
    "\n",
    "loss1_train_s,loss1_test_s,loss2_train_s,loss2_test_s = comparison(y1_train,y1_test,y2_train,y2_test,\n",
    "               y1_hat_train_s,y1_hat_test_s,y2_hat_train_s,y2_hat_test_s,\n",
    "               y1_hat_train_p_s,y1_hat_test_p_s,y2_hat_train_p_s,y2_hat_test_p_s)\n",
    "\n",
    "\n",
    "# Calculate cross-entropy loss on both normalized datasets for train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.b (iv)\n",
    "#Would the results change using standardized data instead of normalized data? No need to run any code here, only explain your reasoning.\n",
    "## --> nope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "id": "7xTDD8SNZhZz"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.c\n",
    "import matplotlib.pyplot as plt\n",
    "# To calculate the curves, it is fine to take 100 threshold values c, i.e.,\n",
    "k = 100\n",
    "ths = np.linspace(0, 1, k)\n",
    "\n",
    "# To approximately calculate the AUC, it is fine to simply use Riemann sums.\n",
    "# This means, if you have 100 (a_i, b_i) pairs for the curves, a_1 <= a_2 <= ...\n",
    "# then you may simply use the sum\n",
    "# sum_{i=1}^99 (b_i + b_{i+1})/2 * (a_{i+1}-a_i)\n",
    "# as the approximation of the integral (or AUC)\n",
    "\n",
    "\n",
    "def confusion_mat_base(y,y_hat):\n",
    "    positive, negative = sum((y==1)), sum((y==0))\n",
    "\n",
    "    tp = sum((y==1)&(y_hat==1))\n",
    "    tn = sum((y==0)&(y_hat==0))\n",
    "    fp = sum((y==0)&(y_hat==1))\n",
    "    fn = sum((y==1)&(y_hat==0))\n",
    "    \n",
    "    return positive,negative,tp,tn,fp,fn\n",
    "\n",
    "def results(y1_train,y1_test,y1_hat_train_p,y1_hat_test_p,\n",
    "            y2_train,y2_test,y2_hat_train_p,y2_hat_test_p,\n",
    "            ths):\n",
    "    \n",
    "    \n",
    "    y1_res = {}\n",
    "    y2_res = {}\n",
    "\n",
    "    y1_res_train = {}\n",
    "    y1_res_test = {}\n",
    "    y2_res_train = {}\n",
    "    y2_res_test = {}\n",
    "\n",
    "    y1_preds_train, y1_preds_test = [],[]\n",
    "    y2_preds_train, y2_preds_test = [],[]\n",
    "    conf1_train, conf1_test = {},{}\n",
    "    conf2_train, conf2_test = {},{}\n",
    "\n",
    "    positive_train1_ls,negative_train1_ls,tp_train1_ls,tn_train1_ls,fp_train1_ls,fn_train1_ls = [],[],[],[],[],[]\n",
    "    positive_test1_ls,negative_test1_ls,tp_test1_ls,tn_test1_ls,fp_test1_ls,fn_test1_ls = [],[],[],[],[],[]\n",
    "    positive_train2_ls,negative_train2_ls,tp_train2_ls,tn_train2_ls,fp_train2_ls,fn_train2_ls = [],[],[],[],[],[]\n",
    "    positive_test2_ls,negative_test2_ls,tp_test2_ls,tn_test2_ls,fp_test2_ls,fn_test2_ls = [],[],[],[],[],[]\n",
    "\n",
    "    y1_loss_train, y1_loss_test = [],[]\n",
    "    y2_loss_train, y2_loss_test = [],[]\n",
    "\n",
    "    for theta in ths:   \n",
    "        y1_hat_train, y1_hat_test = class_assign(y1_hat_train_p,theta),class_assign(y1_hat_test_p,theta)\n",
    "        y2_hat_train, y2_hat_test = class_assign(y2_hat_train_p,theta),class_assign(y2_hat_test_p,theta)\n",
    "        positive_train1,negative_train1,tp_train1,tn_train1,fp_train1,fn_train1 = confusion_mat_base(y1_train,y1_hat_train)\n",
    "        positive_test1,negative_test1,tp_test1,tn_test1,fp_test1,fn_test1 = confusion_mat_base(y1_test,y1_hat_test)\n",
    "        positive_train2,negative_train2,tp_train2,tn_train2,fp_train2,fn_train2 = confusion_mat_base(y2_train,y2_hat_train)\n",
    "        positive_test2,negative_test2,tp_test2,tn_test2,fp_test2,fn_test2 = confusion_mat_base(y2_test,y2_hat_test)\n",
    "\n",
    "        loss1_train,loss1_test,loss2_train,loss2_test = comparison(y1_train,y1_test,y2_train,y2_test,\n",
    "                                                                   y1_hat_train,y1_hat_test,y2_hat_train,y2_hat_test,\n",
    "                                                                   y1_hat_train_p,y1_hat_test_p,y2_hat_train_p,y2_hat_test_p,show=False)\n",
    "        \n",
    "        y1_preds_train.append(y1_hat_train)\n",
    "        y1_preds_test.append(y1_hat_test)\n",
    "        y2_preds_train.append(y2_hat_train)\n",
    "        y2_preds_test.append(y2_hat_test)\n",
    "\n",
    "        positive_train1_ls.append(positive_train1)\n",
    "        negative_train1_ls.append(negative_train1)\n",
    "        tp_train1_ls.append(tp_train1)\n",
    "        tn_train1_ls.append(tn_train1)\n",
    "        fp_train1_ls.append(fp_train1)\n",
    "        fn_train1_ls.append(fn_train1)\n",
    "\n",
    "        positive_test1_ls.append(positive_test1)\n",
    "        negative_test1_ls.append(negative_test1)\n",
    "        tp_test1_ls.append(tp_test1)\n",
    "        tn_test1_ls.append(tn_test1)\n",
    "        fp_test1_ls.append(fp_test1)\n",
    "        fn_test1_ls.append(fn_test1)\n",
    "        \n",
    "\n",
    "        positive_train2_ls.append(positive_train2)\n",
    "        negative_train2_ls.append(negative_train2)\n",
    "        tp_train2_ls.append(tp_train2)\n",
    "        tn_train2_ls.append(tn_train2)\n",
    "        fp_train2_ls.append(fp_train2)\n",
    "        fn_train2_ls.append(fn_train2)\n",
    "\n",
    "        positive_test2_ls.append(positive_test2)\n",
    "        negative_test2_ls.append(negative_test2)\n",
    "        tp_test2_ls.append(tp_test2)\n",
    "        tn_test2_ls.append(tn_test2)\n",
    "        fp_test2_ls.append(fp_test2)\n",
    "        fn_test2_ls.append(fn_test2)\n",
    "\n",
    "\n",
    "        y1_loss_train.append(loss1_train)\n",
    "        y1_loss_test.append(loss1_test)\n",
    "        y2_loss_train.append(loss2_train)\n",
    "        y2_loss_test.append(loss2_test)\n",
    "\n",
    "    conf1_train[\"Positive\"] = positive_train1_ls\n",
    "    conf1_train[\"Negative\"] = negative_train1_ls\n",
    "    conf1_train[\"TP\"] = tp_train1_ls\n",
    "    conf1_train[\"TN\"] = tn_train1_ls\n",
    "    conf1_train[\"FP\"] = fp_train1_ls\n",
    "    conf1_train[\"FN\"] = fn_train1_ls\n",
    "\n",
    "    conf1_test[\"Positive\"] = positive_test1_ls\n",
    "    conf1_test[\"Negative\"] = negative_test1_ls\n",
    "    conf1_test[\"TP\"] = tp_test1_ls\n",
    "    conf1_test[\"TN\"] = tn_test1_ls\n",
    "    conf1_test[\"FP\"] = fp_test1_ls\n",
    "    conf1_test[\"FN\"] = fn_test1_ls\n",
    "\n",
    "    conf2_train[\"Positive\"] = positive_train2_ls\n",
    "    conf2_train[\"Negative\"] = negative_train2_ls\n",
    "    conf2_train[\"TP\"] = tp_train2_ls\n",
    "    conf2_train[\"TN\"] = tn_train2_ls\n",
    "    conf2_train[\"FP\"] = fp_train2_ls\n",
    "    conf2_train[\"FN\"] = fn_train2_ls\n",
    "\n",
    "    conf2_test[\"Positive\"] = positive_test2_ls\n",
    "    conf2_test[\"Negative\"] = negative_test2_ls\n",
    "    conf2_test[\"TP\"] = tp_test2_ls\n",
    "    conf2_test[\"TN\"] = tn_test2_ls\n",
    "    conf2_test[\"FP\"] = fp_test2_ls\n",
    "    conf2_test[\"FN\"] = fn_test2_ls\n",
    "\n",
    "    \n",
    "    y1_res_train[\"Confusion Matrix Base\"] = conf1_train\n",
    "    y1_res_test[\"Confusion Matrix Base\"] = conf1_test\n",
    "    y2_res_train[\"Confusion Matrix Base\"] = conf2_train\n",
    "    y2_res_test[\"Confusion Matrix Base\"] = conf2_test\n",
    "\n",
    "    y1_res_train[\"Loss\"] = y1_loss_train\n",
    "    y1_res_test[\"Loss\"] = y1_loss_test \n",
    "    y2_res_train[\"Loss\"] = y2_loss_train\n",
    "    y2_res_test[\"Loss\"] = y2_loss_test \n",
    "\n",
    "    \n",
    "    y1_res_train[\"Prediction\"] = y1_preds_train\n",
    "    y1_res_test[\"Prediction\"] = y1_preds_test \n",
    "    y2_res_train[\"Prediction\"] = y2_preds_train\n",
    "    y2_res_test[\"Prediction\"] = y2_preds_test \n",
    "\n",
    "    y1_res[\"Test\"] = y1_res_test\n",
    "    y1_res[\"Train\"] = y1_res_train\n",
    "    y2_res[\"Test\"] = y2_res_test\n",
    "    y2_res[\"Train\"] = y2_res_train\n",
    "\n",
    "    return y1_res,y2_res\n",
    "\n",
    "  \n",
    "y1_res,y2_res = results(y1_train,y1_test,y1_hat_train_p,y1_hat_test_p,\n",
    "                        y2_train,y2_test,y2_hat_train_p,y2_hat_test_p,\n",
    "                        ths)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# first data set & logistic regression:\n",
    "# (the code should be reusable for all cases, only exchanging datasets and predicted probabilities depending on the model)\n",
    "\n",
    "# Calculate positives (only depending on the dataset)\n",
    "\n",
    "# Calculate true positives for all threshold values\n",
    "\n",
    "# Calculate false positives for all threshold values\n",
    "\n",
    "# Calculate FDR and TPR rate (points on the FDR/TPR curve) and the AUC\n",
    "\n",
    "\n",
    "# second data set & logistic regression:\n",
    "\n",
    "\n",
    "# first data set and rkhs regression:\n",
    "\n",
    "\n",
    "# second data set and rkhs regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "id": "5YvGSeDoc9ZS"
   },
   "outputs": [],
   "source": [
    "# Exercise 3.\n",
    "\n",
    "# Set model parameters and define matrix D\n",
    "\n",
    "\n",
    "# Scenario 1:\n",
    "# Define Portfolio and possible outcomes for this portfolio using matrix D\n",
    "\n",
    "\n",
    "# Plot histogram of profits and losses\n",
    "\n",
    "\n",
    "# Calculate expected profit and losses and 95%-VaR\n",
    "\n",
    "\n",
    "# Scenario 2:\n",
    "# Define Portfolio and possible outcomes using the matrix D and the predicted default probabilities from the logistic regression model\n",
    "\n",
    "\n",
    "# Plot histogram of profits and losses\n",
    "\n",
    "\n",
    "# Calculate expected profit and losses and 95%-VaR\n",
    "\n",
    "\n",
    "# Scenario 3:\n",
    "# Define Portfolio and possible outcomes using the matrix D and the predicted default probabilities from the rkhs model\n",
    "\n",
    "\n",
    "# Plot histogram of profits & losses (which is simply the performance of each strategy in scenario k, i.e. a profit if positive or a loss if negative)\n",
    "\n",
    "\n",
    "# Calculate expected profit & losses and 95%-VaR for each strategy"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
