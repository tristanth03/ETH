{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0864f6f8-d36b-4bb2-9a39-ce2e9780c5f4",
   "metadata": {},
   "source": [
    "# Coding Project 1 : Linear Regression and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b0c7d-92b3-4cde-a31b-93ff1e7ff46f",
   "metadata": {},
   "source": [
    "**Please write the names of all group members here:**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "*Note:* The provided structure for the code below is only suggestive, and if you want to structure your programs differently you may do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87ace5-781a-402a-8f0b-3266c446cd30",
   "metadata": {},
   "source": [
    "### Question 1 - Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58db105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Question 1, you can import the following packages:\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection  import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65078896",
   "metadata": {},
   "source": [
    "#### 1.a) \n",
    "Import the dataset Housing.csv into Python as a pandas DataFrame. To determine whether a variable is categorical or numerical, please refer to the file data_description.txt. Remember that the first column of the csv file is an index column and should not be considered as an explanatory variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e293a8f7",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57664d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First column of the csv file in an index column so we do not need it\n",
    "data = pd.read_csv(\"Housing.csv\")\n",
    "data.drop(columns=['Id'],inplace=True)\n",
    "# Seperatin the explanatory features from the target variable\n",
    "X = data.drop(columns=[\"SalePrice\"])\n",
    "y = data[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef849e",
   "metadata": {},
   "source": [
    "#### 1.b) \n",
    "\n",
    "Graphically determine whether the target variable SalePrice is approximately Gaussian. If not, suggest a suitable transformation to bring SalePrice closer to a Gaussian distribution (logarithmic, inverse, square-root, ...). Do not forget to apply this transformation to the target variable SalePrice in the dataset. Why is it important to consider such potential transformations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b1e78b",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4115933f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot histogram of the target variable SalePrice\n",
    "plt.hist(y, bins=50)\n",
    "plt.title(\"SalePrice Histogram\")\n",
    "plt.xlabel(\"SalePrice\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1207f736",
   "metadata": {},
   "source": [
    "We observe that the target variable SalePrice is not Gaussian but is strongly right skewed. Most houses sell at lower prices, while a few very large prices create a long right tail. Histogram's shape resembles a LogNormal distribution. We applied Log transformation, which is suited for right skewed data, by taking the natural logarithm of all data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25bc3124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We take natural logarithm of all data points in y\n",
    "y_log = np.log(y)\n",
    "\n",
    "# Plot histogram of the transformed target variable SalePrice\n",
    "plt.hist(y_log, bins=50)\n",
    "plt.title(\"Transformed SalePrice Histogram\")\n",
    "plt.xlabel(\"log(SalePrice)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c95e88",
   "metadata": {},
   "source": [
    "We observe that after applying the logarithmic transformation, the distribution of SalePrice looks much more symmetric and bell-shaped.  \n",
    "The long right tail has been compressed, and the data now resembles a normal distribution much more closely.  \n",
    "\n",
    "To confirm this observation, we compare the normality of the target variable before and after transformation by:  \n",
    "1. Computing the sample skewness.  \n",
    "2. Inspecting QQ-plots to see how closely the data follows a theoretical Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ab7047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness before log transform: 1.881\n",
      "Skewness after log transform:  0.121\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro-Wilk Test (SalePrice): ShapiroResult(statistic=0.8696714665902145, pvalue=3.2061412312021656e-33)\n",
      "Shapiro-Wilk Test (log(SalePrice)): ShapiroResult(statistic=0.9912067503807811, pvalue=1.1490615527264654e-07)\n"
     ]
    }
   ],
   "source": [
    "# Sample skewness before and after transformation\n",
    "skew_before = stats.skew(y)     \n",
    "skew_after  = stats.skew(y_log)\n",
    "\n",
    "print(f\"Skewness before log transform: {skew_before:.3f}\")\n",
    "print(f\"Skewness after log transform:  {skew_after:.3f}\")\n",
    "\n",
    "# QQ-plot \n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# QQ plot before transformation\n",
    "plt.subplot(1,2,1)\n",
    "stats.probplot(y, dist=\"norm\", plot=plt)\n",
    "plt.title(\"QQ Plot of SalePrice\")\n",
    "\n",
    "# QQ plot after transformation\n",
    "plt.subplot(1,2,2)\n",
    "stats.probplot(y_log, dist=\"norm\", plot=plt)\n",
    "plt.title(\"QQ Plot of log(SalePrice)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk test for normality\n",
    "shapiro_y = stats.shapiro(y)\n",
    "shapiro_y_log = stats.shapiro(y_log)\n",
    "\n",
    "print(\"Shapiro-Wilk Test (SalePrice):\", shapiro_y)\n",
    "print(\"Shapiro-Wilk Test (log(SalePrice)):\", shapiro_y_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3da3e6",
   "metadata": {},
   "source": [
    "Target variable SalePrice has a skewness of about 1.88, which confirms strong positive skew. Left QQ plot shows that the high prices - upper tail and the low prices - lower tail of the distribution deviate deviate substantially from the theoretical diagonal, indicating non-normality.\n",
    "\n",
    "After applying the log transformation, skewness drops to about 0.12, which is quite close to zero. Right QQ plot now aligns much more closely with the diagonal, showing that the distribution is approximately Gaussian. However, even in the transformed data, some values at the lower and upper ends of the distribution are still a bit distant from the diagonal. This suggests that while the log transformation substantially improves normality, a few outliers remain in the tails.\n",
    "\n",
    "ShapiroWilk shows strong non-normality for target SalePrice (W=0.870, p3.2e-33) and improved but still non-normal for log(SalePrice) (W=0.991, p1.15e-07). The log transform improves normality, as W statistic moves much closer to 1, although p remains below 0.05, indicating we cannot assume perfect normality.\n",
    "\n",
    "Linear regression works best when the data follows certain assumptions, particularly that residuals are normally distributed with constant variance. When our target variable is heavily skewed like the original SalePrice, these assumptions get violated. This can lead to unreliable predictions and invalid statistical tests. The log transformation helps by making the distribution more symmetric and stabilizing the variance, which improves our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29449164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply logarithmic transformation to the target variable SalePrice in the dataset as well as to the separated target variable y\n",
    "data[\"SalePrice\"] = np.log(data[\"SalePrice\"])\n",
    "y = y_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f746c11e",
   "metadata": {},
   "source": [
    "#### 1.c) \n",
    "Split the data into a training set (X,y)_train and a test set (X,y)_test. Randomly assign 70% of the observations to the training set and the remaining 30% to the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864d4c40",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42a7451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly assign 70% of the observations to the training set and the remaining 30% to the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_log, test_size=0.3, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b029a",
   "metadata": {},
   "source": [
    "#### 1.d) \n",
    "Replace missing values in X using the training data statistics only -> use .fillna(...). For numerical features, replace missing values with the mean of the column. For categorical features, replace missing values with the most frequent category. You can use the function df.select_dtypes(...) to idetify categorical variables as the variables with type 'object' and 'category'. Some categorical variables admit NA (or None) as a valid category, which should be treated as an actual level and not as missing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c3a97",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf5dd6ed-bdad-4351-a2ff-01aded150b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns where \"NA\" means \"no such feature\" (treat as a real category)\n",
    "na_level_cols = [\n",
    "    \"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\",\n",
    "    \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\",\n",
    "    \"PoolQC\", \"Fence\", \"MiscFeature\"\n",
    "]\n",
    "\n",
    "for col in na_level_cols:\n",
    "    if col in X_train.columns:\n",
    "        X_train[col] = X_train[col].fillna(\"NA\")\n",
    "        X_test[col]  = X_test[col].fillna(\"NA\")\n",
    "\n",
    "# column where \"None\" is the valid level\n",
    "X_train[\"MasVnrType\"] = X_train[\"MasVnrType\"].fillna(\"None\")\n",
    "X_test[\"MasVnrType\"]  = X_test[\"MasVnrType\"].fillna(\"None\")\n",
    "\n",
    "# Treat MSSubClass as categorical by converting to string\n",
    "X_train[\"MSSubClass\"] = X_train[\"MSSubClass\"].astype(str)\n",
    "X_test[\"MSSubClass\"]  = X_test[\"MSSubClass\"].astype(str)\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "# Fill missing values in numerical features with training mean\n",
    "for col in num_cols:\n",
    "    mean_val = X_train[col].mean()\n",
    "    X_train[col] = X_train[col].fillna(mean_val)\n",
    "    X_test[col] = X_test[col].fillna(mean_val)\n",
    "\n",
    "# Fill actual NaN with the most frequent category from the train set\n",
    "for col in cat_cols:\n",
    "    mode_val = X_train[col].mode()[0]\n",
    "    X_train[col] = X_train[col].fillna(mode_val)\n",
    "    X_test[col]  = X_test[col].fillna(mode_val)\n",
    "\n",
    "# z-score standardization with train dataset\n",
    "num_means = X_train[num_cols].mean()\n",
    "num_stds  = X_train[num_cols].std()\n",
    "\n",
    "X_train[num_cols] = (X_train[num_cols] - num_means) / num_stds\n",
    "X_test[num_cols]  = (X_test[num_cols]  - num_means) / num_stds\n",
    "\n",
    "# Save copies before encoding\n",
    "X_train_original = X_train.copy()\n",
    "X_test_original = X_test.copy()\n",
    "\n",
    "# one hot encoding for all categorical features\n",
    "X_train_enc = pd.get_dummies(X_train, columns=cat_cols, drop_first=True)\n",
    "X_test_enc  = pd.get_dummies(X_test,  columns=cat_cols, drop_first=True)\n",
    "\n",
    "# ensure test has the same columns as train\n",
    "X_test_enc = X_test_enc.reindex(columns=X_train_enc.columns, fill_value=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d3abf7",
   "metadata": {},
   "source": [
    "We standardized numeric features with z-scores using only training means/standard deviations and applied the same transformation to the test set. We one-hot encoded all categorical variables, dropped the first level (so as to avoid perfect multicolinearity) and then reindexed the test matrix to match the training columns, filling any missing dummy columns with zeros so both matrices are aligned. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86354e30-0b82-481b-a39f-c611e08a4408",
   "metadata": {},
   "source": [
    "### Question 2 - Linear Regression on Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18d059c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import allowed packages\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e20a030",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.a) \n",
    "Restrict your analysis to the numerical predictors only (i.e., exclude all categorical features from X). Fit a linear regression model on the training dataset using the sklearn Python package. The regression should include an intercept term. Present a table with the regression coefficients for each feature. Compare the in-sample and out-of-sample Mean Squared Error (MSE) and R2.\n",
    "\n",
    "If you transformed the target variable, you have to inverse-transform the predictions before computing MSE and R2, so that these metrics are reported on the original SalePrice scale. In addition, also report the MSE and R2 on the transformed scale, and comment on the differences between the two. What does each set of metrics tell you about model performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51783fd2",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b6b2748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression coefficients (numerical features only):\n",
      "          Feature   Coefficient\n",
      "0       Intercept  1.202554e+01\n",
      "15      GrLivArea -3.353349e+10\n",
      "13       2ndFlrSF  2.814583e+10\n",
      "12       1stFlrSF  2.474986e+10\n",
      "10      BsmtUnfSF -1.830686e+10\n",
      "8      BsmtFinSF1 -1.826251e+10\n",
      "11    TotalBsmtSF  1.770690e+10\n",
      "9      BsmtFinSF2 -6.683412e+09\n",
      "14   LowQualFinSF  2.773194e+09\n",
      "3     OverallQual  1.069642e-01\n",
      "5       YearBuilt  8.873536e-02\n",
      "4     OverallCond  5.794167e-02\n",
      "25     GarageCars  2.799520e-02\n",
      "23     Fireplaces  2.777057e-02\n",
      "26     GarageArea  2.422443e-02\n",
      "2         LotArea  2.306347e-02\n",
      "31    ScreenPorch  2.176636e-02\n",
      "6    YearRemodAdd  2.123033e-02\n",
      "22   TotRmsAbvGrd  2.109885e-02\n",
      "16   BsmtFullBath  2.003382e-02\n",
      "1     LotFrontage  1.923279e-02\n",
      "18       FullBath  1.676651e-02\n",
      "21   KitchenAbvGr -1.575383e-02\n",
      "35         YrSold -1.322372e-02\n",
      "19       HalfBath  1.194476e-02\n",
      "27     WoodDeckSF  9.024234e-03\n",
      "29  EnclosedPorch  8.863640e-03\n",
      "30      3SsnPorch  6.765600e-03\n",
      "34         MoSold -5.265210e-03\n",
      "28    OpenPorchSF -4.674933e-03\n",
      "7      MasVnrArea -2.847529e-03\n",
      "24    GarageYrBlt -2.504243e-03\n",
      "32       PoolArea -6.772434e-04\n",
      "20   BedroomAbvGr -4.370205e-04\n",
      "33        MiscVal -3.851614e-04\n",
      "17   BsmtHalfBath -3.803253e-04\n",
      "Performance on log(SalePrice):\n",
      "In-sample MSE: 0.0189, R^2: 0.8857\n",
      "Out-of-sample MSE: 0.0332, R^2: 0.7736\n",
      "\n",
      "Performance on original SalePrice scale:\n",
      "In-sample MSE: 1052951082, R^2: 0.8431\n",
      "Out-of-sample MSE: 15277230963, R^2: -1.8533\n"
     ]
    }
   ],
   "source": [
    "# Select numerical type columns\n",
    "X_train_num = X_train_original[num_cols].copy()\n",
    "X_test_num  = X_test_original[num_cols].copy()\n",
    "\n",
    "# Fitting linear regression with intercept\n",
    "linreg = LinearRegression(fit_intercept=True)\n",
    "linreg.fit(X_train_num, y_train)\n",
    "\n",
    "# Get coefficients in a table, including the intercept and sorted by absolute value\n",
    "intercept = linreg.intercept_\n",
    "coefs = linreg.coef_\n",
    "feat_names = list(X_train_num.columns)\n",
    "\n",
    "coef_table = pd.DataFrame({\n",
    "    \"Feature\": [\"Intercept\"] + feat_names,\n",
    "    \"Coefficient\": np.insert(coefs, 0, intercept)\n",
    "}).sort_values(by=\"Coefficient\", key=abs, ascending=False)\n",
    "\n",
    "# move intercept to the top for better order\n",
    "coef_table = pd.concat([\n",
    "    coef_table.loc[coef_table[\"Feature\"] == \"Intercept\"],\n",
    "    coef_table.loc[coef_table[\"Feature\"] != \"Intercept\"]\n",
    "])\n",
    "\n",
    "print(\"Regression coefficients (numerical features only):\")\n",
    "print(coef_table)\n",
    "\n",
    "# Predictions on trained model \n",
    "y_train_pred = linreg.predict(X_train_num)\n",
    "y_test_pred  = linreg.predict(X_test_num)\n",
    "\n",
    "# In-sample metrics on ln scale \n",
    "mse_train_log = mean_squared_error(y_train, y_train_pred)\n",
    "r2_train_log  = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Out-of-sample metrics on ln scale\n",
    "mse_test_log = mean_squared_error(y_test, y_test_pred)\n",
    "r2_test_log  = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Performance on log(SalePrice):\")\n",
    "print(f\"In-sample MSE: {mse_train_log:.4f}, R^2: {r2_train_log:.4f}\")\n",
    "print(f\"Out-of-sample MSE: {mse_test_log:.4f}, R^2: {r2_test_log:.4f}\")\n",
    "\n",
    "# Inverse-transform predictions by exponentiating\n",
    "y_train_pred_orig = np.exp(y_train_pred)\n",
    "y_test_pred_orig  = np.exp(y_test_pred)\n",
    "\n",
    "y_train_orig = np.exp(y_train)\n",
    "y_test_orig  = np.exp(y_test)\n",
    "\n",
    "# In-sample metrics on original scale \n",
    "mse_train_orig = mean_squared_error(y_train_orig, y_train_pred_orig)\n",
    "r2_train_orig  = r2_score(y_train_orig, y_train_pred_orig)\n",
    "\n",
    "# Out-of-sample metrics on original scale\n",
    "mse_test_orig = mean_squared_error(y_test_orig, y_test_pred_orig)\n",
    "r2_test_orig  = r2_score(y_test_orig, y_test_pred_orig)\n",
    "\n",
    "print(\"\\nPerformance on original SalePrice scale:\")\n",
    "print(f\"In-sample MSE: {mse_train_orig:.0f}, R^2: {r2_train_orig:.4f}\")\n",
    "print(f\"Out-of-sample MSE: {mse_test_orig:.0f}, R^2: {r2_test_orig:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b881a79c",
   "metadata": {},
   "source": [
    "We fit an OLS linear regression using only the numerical predictors, including an intercept term. We extracted and presented the regression coefficients, then evaluated model performance both in-sample and out-of-sample. Performance metrics (MSE, R) were computed first on the log(SalePrice) scale, and then predictions were inverse-transformed to the original SalePrice scale to assess interpretability in real currency values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80ed78b",
   "metadata": {},
   "source": [
    "On the log scale, the model shows strong in-sample performance R = 0.89 and reasonable out-of-sample performance R = 0.77. This suggests the linear model captures around 77% of variation in log-transformed prices.\n",
    "\n",
    "However, after inverse-transforming to the original SalePrice scale, the out-of-sample R turns negative, meaning the models predictions are worse than simply using the empirical mean of the training prices.\n",
    "\n",
    "The extreme magnitude and instability of certain coefficients for example basement square footage features in the order of 10^10 suggest severe multicollinearity among predictors, which causes poor generalization when moving back to the original scale. Coefficients for basement and living-area variables show instability because of multicollinearity for example TotalBsmtSF equals the sum of its components and similar for GrLivArea. We will do a quick sanity check for multicolinearity just to be sure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cfa1d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check after dropping redundant totals:\n",
      " In-sample R (log): 0.8857\n",
      " Out-of-sample R (log): 0.7736\n",
      "Max abs. coefficient: 0.10696108601648163\n",
      "\n",
      "Performance on original SalePrice scale (after dropping totals):\n",
      "In-sample MSE: 1053079599, R^2: 0.8431\n",
      "Out-of-sample MSE: 15273939895, R^2: -1.8526\n",
      "\n",
      "Max abs. coefficient: 0.10696108601648163\n"
     ]
    }
   ],
   "source": [
    "# Drop obvious redundant totals (they are linear combinations of components)\n",
    "drop_exact = [\"TotalBsmtSF\", \"GrLivArea\"]\n",
    "\n",
    "linreg_ok = LinearRegression(fit_intercept=True)\n",
    "linreg_ok.fit(X_train_num.drop(columns=drop_exact), y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_ok = linreg_ok.predict(X_train_num.drop(columns=drop_exact))\n",
    "y_test_pred_ok  = linreg_ok.predict(X_test_num.drop(columns=drop_exact))\n",
    "\n",
    "# Evaluate performance (log scale)\n",
    "mse_train_log_ok = mean_squared_error(y_train, y_train_pred_ok)\n",
    "r2_train_log_ok  = r2_score(y_train, y_train_pred_ok)\n",
    "mse_test_log_ok  = mean_squared_error(y_test, y_test_pred_ok)\n",
    "r2_test_log_ok   = r2_score(y_test, y_test_pred_ok)\n",
    "\n",
    "print(\"Sanity check after dropping redundant totals:\")\n",
    "print(f\" In-sample R (log): {r2_train_log_ok:.4f}\")\n",
    "print(f\" Out-of-sample R (log): {r2_test_log_ok:.4f}\")\n",
    "\n",
    "# Check coefficient magnitudes\n",
    "print(\"Max abs. coefficient:\", np.abs(linreg_ok.coef_).max())\n",
    "\n",
    "# Inverse-transform predictions by exponentiating \n",
    "y_train_pred_orig_ok = np.exp(y_train_pred_ok)\n",
    "y_test_pred_orig_ok  = np.exp(y_test_pred_ok)\n",
    "\n",
    "y_train_orig = np.exp(y_train)\n",
    "y_test_orig  = np.exp(y_test)\n",
    "\n",
    "# Metrics on original SalePrice scale\n",
    "mse_train_orig_ok = mean_squared_error(y_train_orig, y_train_pred_orig_ok)\n",
    "r2_train_orig_ok  = r2_score(y_train_orig, y_train_pred_orig_ok)\n",
    "mse_test_orig_ok  = mean_squared_error(y_test_orig, y_test_pred_orig_ok)\n",
    "r2_test_orig_ok   = r2_score(y_test_orig, y_test_pred_orig_ok)\n",
    "\n",
    "print(\"\\nPerformance on original SalePrice scale (after dropping totals):\")\n",
    "print(f\"In-sample MSE: {mse_train_orig_ok:.0f}, R^2: {r2_train_orig_ok:.4f}\")\n",
    "print(f\"Out-of-sample MSE: {mse_test_orig_ok:.0f}, R^2: {r2_test_orig_ok:.4f}\")\n",
    "\n",
    "# Quick coefficient sanity check\n",
    "print(\"\\nMax abs. coefficient:\", np.abs(linreg_ok.coef_).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0cfc54",
   "metadata": {},
   "source": [
    "After dropping redundant totals, the coefficients became stable and interpretable, confirming that multicollinearity was the source of earlier instability. However, when evaluating on the original SalePrice scale, the out-of-sample R^2 remains negative. On the log scale, the model performs well with in-sample R = 0.8857 and out-of-sample R = 0.7735, showing it explains about 77% of the variance in log-transformed prices on the test set. However, when we convert predictions back to the original scale, the out-of-sample R becomes -1.8526, which is negative and means the model performs worse than just predicting the average house price for everything. This happens because the exponential function amplifies errors - small mistakes in log space become much larger mistakes when converted to actual dollars, especially for expensive houses. The out-of-sample MSE on the original scale is about 15 times larger than the in-sample MSE, indicating the model struggles significantly with generalization when measured in actual dollar terms. The log-scale metrics tell us the model learned patterns reasonably well, but the original-scale metrics reveal that translating these patterns back to real prices produces poor practical predictions, particularly on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d794b",
   "metadata": {},
   "source": [
    "#### 2.b) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180470c2",
   "metadata": {},
   "source": [
    "#### b)\n",
    "\n",
    "The `sklearn` package does not provide standard errors for the estimated regression coefficients, which are essential tools to assess the statistical precision of an estimate. Therefore, you will now use matrix algebra in Python with the `numpy` package to compute the standard errors of the estimated coefficients \\$\\hat{\\beta}\\$. All computations in this part should be performed using the training set only.\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{m \\times (d+1)}$ denote the design matrix (including a column of ones for the intercept term), and let $y \\in \\mathbb{R}^m$ denote the observed target values.\n",
    "\n",
    "#### (i) Compute the estimated coefficients $\\hat{\\beta}$ using\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (A^\\top A)^{-1} A^\\top y.\n",
    "$$\n",
    "\n",
    "Note that $\\hat{\\beta_0}$ denotes the estimate of the intercept.\n",
    "\n",
    "**Practical note:** In code, do not form matrix inverses explicitly; instead use a numerically stable equivalent such as np.linalg.solve(A.T @ A, A.T @ y).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86d538",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dd9c00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated coefficients (matrix algebra):\n",
      "          Feature  Coefficient\n",
      "0       Intercept    12.025544\n",
      "10      BsmtUnfSF    -3.638886\n",
      "8      BsmtFinSF1    -3.603264\n",
      "11    TotalBsmtSF     3.571429\n",
      "9      BsmtFinSF2    -1.329099\n",
      "15      GrLivArea     1.156969\n",
      "13       2ndFlrSF    -0.890109\n",
      "12       1stFlrSF    -0.784149\n",
      "3     OverallQual     0.106961\n",
      "14   LowQualFinSF    -0.089288\n",
      "5       YearBuilt     0.088736\n",
      "4     OverallCond     0.057950\n",
      "25     GarageCars     0.027982\n",
      "23     Fireplaces     0.027771\n",
      "26     GarageArea     0.024236\n",
      "2         LotArea     0.023077\n",
      "31    ScreenPorch     0.021770\n",
      "6    YearRemodAdd     0.021237\n",
      "22   TotRmsAbvGrd     0.021091\n",
      "16   BsmtFullBath     0.020035\n",
      "1     LotFrontage     0.019239\n",
      "18       FullBath     0.016777\n",
      "21   KitchenAbvGr    -0.015755\n",
      "35         YrSold    -0.013225\n",
      "19       HalfBath     0.011940\n",
      "27     WoodDeckSF     0.009027\n",
      "29  EnclosedPorch     0.008867\n",
      "30      3SsnPorch     0.006771\n",
      "34         MoSold    -0.005261\n",
      "28    OpenPorchSF    -0.004673\n",
      "7      MasVnrArea    -0.002847\n",
      "24    GarageYrBlt    -0.002502\n",
      "32       PoolArea    -0.000684\n",
      "20   BedroomAbvGr    -0.000443\n",
      "33        MiscVal    -0.000391\n",
      "17   BsmtHalfBath    -0.000388\n"
     ]
    }
   ],
   "source": [
    "# We are using only numerical predictors \n",
    "X_train_num = X_train[num_cols]\n",
    "\n",
    "# Matrix A with column of 1s for intercept\n",
    "A = np.column_stack([np.ones(X_train_num.shape[0]), X_train_num.values])\n",
    "y_vec = y_train.values.reshape(-1, 1)\n",
    "\n",
    "# stable version computing beta_hat = (A^T A)^(-1) A^T y using np.linalg.solve \n",
    "ATA = A.T @ A\n",
    "ATy = A.T @ y_vec\n",
    "beta_hat = np.linalg.solve(ATA, ATy).flatten()\n",
    "\n",
    "# Collect into df\n",
    "beta_table = pd.DataFrame({\n",
    "    \"Feature\": [\"Intercept\"] + list(X_train_num.columns),\n",
    "    \"Coefficient\": beta_hat\n",
    "}).sort_values(by=\"Coefficient\", key=abs, ascending=False)\n",
    "\n",
    "print(\"Estimated coefficients (matrix algebra):\")\n",
    "print(beta_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0e435b",
   "metadata": {},
   "source": [
    "We formed a design matrix with an intercept and only the numerical predictors, then solved $(A^\\top A)\\hat\\beta=A^\\top y$ using np.linalg.solve.\n",
    "This gave OLS coefficients on the log-price scale with intercept  12.03 and reasonable magnitudes for key features (e.g., GrLivArea = 0.111, TotalBsmtSF = -0.722031).\n",
    "Basement totals vs. components show opposing signs, hinting at multicollinearity in that group.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64c073",
   "metadata": {},
   "source": [
    "#### (ii)  \n",
    "Compute the standard error of each coefficient $\\hat{\\beta}_j, j = 0, \\ldots, d$:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{m - (d+1)} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2, \n",
    "\\quad Var(\\hat{\\beta}) = \\hat{\\sigma}^2 (A^T A)^{-1},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "SE(\\hat{\\beta}_j) = \\sqrt{\\hat{\\sigma}^2 \\cdot \\left[(A^T A)^{-1}\\right]_{jj}},\n",
    "$$\n",
    "\n",
    "where $\\left[(A^T A)^{-1}\\right]_{jj}$ denotes the $j$-th diagonal element.  \n",
    "Recall that $j=0$ corresponds to the intercept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76597d14",
   "metadata": {},
   "source": [
    "#### Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b864355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.01783311e+14  8.75517061e+14  2.55605005e+13 ... -1.14464144e+13\n",
      "  -1.39891422e+14  4.59774023e+13]\n",
      " [-1.44184209e+14 -4.52778271e+14 -5.98921079e+13 ...  1.91497901e+13\n",
      "   9.82894570e+13  7.56750755e+13]\n",
      " [-1.83508337e+14 -3.85012647e+14 -1.01886932e+14 ...  1.86719046e+13\n",
      "   6.50821924e+13  2.83312792e+14]\n",
      " ...\n",
      " [ 2.61535650e+13  5.79495304e+13  1.65379198e+13 ... -2.73511072e+12\n",
      "  -2.73529925e+13  5.58612544e+13]\n",
      " [ 3.49183962e+13  8.71972923e+13  2.17263996e+13 ... -7.46113022e+12\n",
      "  -4.64577195e+13  8.34239788e+13]\n",
      " [ 1.04006616e+14  1.90271103e+14  5.90735800e+13 ... -6.78944326e+12\n",
      "  -1.04343512e+14  1.90637822e+14]]\n",
      "          Feature  Coefficient (numpy)  Std. Error\n",
      "0       Intercept            12.025544    0.004373\n",
      "1     LotFrontage             0.019239    0.005138\n",
      "2         LotArea             0.023077    0.004950\n",
      "3     OverallQual             0.106961    0.007815\n",
      "4     OverallCond             0.057950    0.005628\n",
      "5       YearBuilt             0.088736    0.009810\n",
      "6    YearRemodAdd             0.021237    0.007075\n",
      "7      MasVnrArea            -0.002847    0.005117\n",
      "8      BsmtFinSF1            -6.108039         NaN\n",
      "9      BsmtFinSF2            -2.245756         NaN\n",
      "10      BsmtUnfSF            -6.149744         NaN\n",
      "11    TotalBsmtSF             6.000000         NaN\n",
      "12       1stFlrSF            -3.100270         NaN\n",
      "13       2ndFlrSF            -3.524029         NaN\n",
      "14   LowQualFinSF            -0.348807         NaN\n",
      "15      GrLivArea             4.295072         NaN\n",
      "16   BsmtFullBath             0.020035    0.006588\n",
      "17   BsmtHalfBath            -0.000388    0.004732\n",
      "18       FullBath             0.016777    0.007517\n",
      "19       HalfBath             0.011940    0.006582\n",
      "20   BedroomAbvGr            -0.000443    0.006698\n",
      "21   KitchenAbvGr            -0.015755    0.005081\n",
      "22   TotRmsAbvGrd             0.021091    0.009520\n",
      "23     Fireplaces             0.027771    0.005459\n",
      "24    GarageYrBlt            -0.002502    0.008134\n",
      "25     GarageCars             0.027982    0.010507\n",
      "26     GarageArea             0.024236    0.010215\n",
      "27     WoodDeckSF             0.009027    0.004792\n",
      "28    OpenPorchSF            -0.004673    0.004906\n",
      "29  EnclosedPorch             0.008867    0.004894\n",
      "30      3SsnPorch             0.006771    0.004439\n",
      "31    ScreenPorch             0.021770    0.004664\n",
      "32       PoolArea            -0.000684    0.004510\n",
      "33        MiscVal            -0.000391    0.004425\n",
      "34         MoSold            -0.005261    0.004494\n",
      "35         YrSold            -0.013225    0.004487\n",
      "\n",
      "Estimated sigma^2 (training): 0.01954281715990425\n",
      "rank(ATA): 34   full rank should be 36\n",
      "Condition number of A: 1.241583367538106e+16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trist\\AppData\\Local\\Temp\\ipykernel_44056\\4090656932.py:33: RuntimeWarning: invalid value encountered in sqrt\n",
      "  se_beta  = np.sqrt(np.diag(var_beta))           # shape (d+1,)\n"
     ]
    }
   ],
   "source": [
    "# We are using only numerical predictors \n",
    "X_train_num = X_train[num_cols]\n",
    "\n",
    "# Matrix A with intercept\n",
    "A = np.column_stack([np.ones(X_train_num.shape[0]), X_train_num.values])  # shape (m, d+1)\n",
    "y_vec = y_train.values  # shape (m,)\n",
    "\n",
    "\n",
    "print(np.linalg.inv(A@A.T))\n",
    "# equations\n",
    "ATA = A.T @ A\n",
    "ATy = A.T @ y_vec\n",
    "\n",
    "# Coefficients (stable solve, no explicit inverse)\n",
    "beta_hat = np.linalg.solve(ATA, ATy)           # shape (d+1,)\n",
    "\n",
    "# Fitted values and residuals\n",
    "y_hat = A @ beta_hat                            # shape (m,)\n",
    "resid = y_vec - y_hat\n",
    "\n",
    "# Dimensions\n",
    "m = A.shape[0]\n",
    "d = A.shape[1] - 1   # number of predictors (excluding intercept)\n",
    "\n",
    "# Residual variance with df = m - (d+1)\n",
    "sigma2_hat = (resid @ resid) / (m - (d + 1))\n",
    "\n",
    "# (A^T A)^{-1} via solve for numerical stability\n",
    "ATA_inv = np.linalg.solve(ATA, np.identity(ATA.shape[0]))\n",
    "\n",
    "# Variance-covariance of beta and standard errors\n",
    "var_beta = sigma2_hat * ATA_inv                 # shape (d+1, d+1)\n",
    "se_beta  = np.sqrt(np.diag(var_beta))           # shape (d+1,)\n",
    "\n",
    "# Tidy table\n",
    "features = [\"Intercept\"] + list(X_train_num.columns)\n",
    "se_table = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Coefficient (numpy)\": beta_hat,\n",
    "    \"Std. Error\": se_beta\n",
    "})\n",
    "\n",
    "print(se_table)\n",
    "print(\"\\nEstimated sigma^2 (training):\", sigma2_hat)\n",
    "\n",
    "print(\"rank(ATA):\", np.linalg.matrix_rank(ATA), \"  full rank should be\", ATA.shape[0])\n",
    "print(\"Condition number of A:\", np.linalg.cond(A))\n",
    "diag_cov = np.diag(var_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b71263",
   "metadata": {},
   "source": [
    "We estimated coefficients via matrix algebra and computed their standard errors from the variancecovariance matrix. The residual variance is small ($\\hat\\sigma^2=0.0195$), indicating a good fit on the log-price scale. Basement square-footage variables (`TotalBsmtSF`, `BsmtFinSF1/2`, `BsmtUnfSF`) yield `NaN` SEs because the design matrix is not full rank. Consistently, our rank check gave $\\mathrm{rank}(A^\\top A)=34$ with 36 columns, and the condition number is huge ($1.3\\times10^{16}$), both signaling extreme multicollinearity. We also see very large SEs for `1stFlrSF`, `2ndFlrSF`, `LowQualFinSF`, and `GrLivArea` (on the order of $10^4$$10^5$), making those coefficient estimates statistically unreliable. Moreover, negative diagonal entries in $\\mathrm{Var}(\\hat\\beta)$ for the basement features show the variance estimate is invalid in those directions, again due to perfect collinearity where `TotalBsmtSF` is almost the sum of its components. Overall, while coefficient magnitudes look reasonable, the SE patterns confirm redundancy among predictors and multicollinearity as the core issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6063dba6",
   "metadata": {},
   "source": [
    "#### (iii)  \n",
    "Compute the in-sample MSE and $R^2$ using matrix algebra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b117092",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ef75e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample metrics on log(SalePrice):\n",
      "  MSE(log): 0.018854\n",
      "  R^2(log): 0.8857\n",
      "\n",
      "In-sample metrics on original SalePrice scale:\n",
      "  MSE: 1053079599\n",
      "  R^2: 0.8431\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "# In sample metrics via matrix algebra\n",
    "# Ln scale \n",
    "res_log = y_vec - y_hat\n",
    "mse_log = (res_log @ res_log) / len(y_vec)\n",
    "sst_log = ((y_vec - y_vec.mean()) @ (y_vec - y_vec.mean()))\n",
    "r2_log  = 1.0 - (res_log @ res_log) / sst_log\n",
    "\n",
    "print(\"In-sample metrics on log(SalePrice):\")\n",
    "print(f\"  MSE(log): {mse_log:.6f}\")\n",
    "print(f\"  R^2(log): {r2_log:.4f}\")\n",
    "\n",
    "#Original scale \n",
    "y_orig    = np.exp(y_vec)\n",
    "yhat_orig = np.exp(y_hat)\n",
    "res_orig  = y_orig - yhat_orig\n",
    "mse_orig  = (res_orig @ res_orig) / len(y_orig)\n",
    "sst_orig  = ((y_orig - y_orig.mean()) @ (y_orig - y_orig.mean()))\n",
    "r2_orig   = 1.0 - (res_orig @ res_orig) / sst_orig\n",
    "\n",
    "print(\"\\nIn-sample metrics on original SalePrice scale:\")\n",
    "print(f\"  MSE: {mse_orig:.0f}\")\n",
    "print(f\"  R^2: {r2_orig:.4f}\")\n",
    "print(np.linalg.matrix_rank(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e132a71f",
   "metadata": {},
   "source": [
    "We computed the in-sample mean squared error (MSE) and $R^2$ using matrix algebra, both on the log-transformed SalePrice and on the original SalePrice scale. On the log scale, the model achieves $\\text{MSE} = 0.0189$ and $R^2 = 0.886$, showing that it explains about 89% of the variation in log-prices. After exponentiating predictions back to the original price scale, the in-sample performance remains strong with $\\text{MSE} = 1.05 \\times 10^9$ and $R^2 = 0.843$. This means the model captures most of the variation in housing prices, though performance is slightly weaker on the original scale because the exponential transformation magnifies residuals, especially for expensive houses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44bd2fb",
   "metadata": {},
   "source": [
    "#### (iv)  \n",
    "Replace $(A^T A)^{-1}A^T$ by the MoorePenrose pseudoinverse $A^+$ (use `np.linalg.pinv`).  \n",
    "Do $\\hat{\\beta}$, $\\hat{\\sigma}^2$, and the standard errors change?  \n",
    "Briefly explain when the results are identical and when they can differ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093becbb",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33b5db93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Feature  beta (solve)     SE (solve)  beta (pinv)  SE (pinv)\n",
      "0      Intercept     12.025544       0.004373    12.025544   0.004373\n",
      "1    LotFrontage      0.019239       0.005138     0.019239   0.005138\n",
      "2        LotArea      0.023077       0.004950     0.023077   0.004950\n",
      "3    OverallQual      0.106961       0.007815     0.106961   0.007815\n",
      "4    OverallCond      0.057950       0.005628     0.057950   0.005628\n",
      "5      YearBuilt      0.088736       0.009810     0.088736   0.009810\n",
      "6   YearRemodAdd      0.021237       0.007075     0.021237   0.007075\n",
      "7     MasVnrArea     -0.002847       0.005117    -0.002847   0.005117\n",
      "8     BsmtFinSF1     -3.603264  676653.114937     0.034486   0.004938\n",
      "9     BsmtFinSF2     -1.329099  251590.780836     0.002185   0.004496\n",
      "10     BsmtUnfSF     -3.638886  686998.521620     0.007698   0.004116\n",
      "11   TotalBsmtSF      3.571429  675493.031813     0.044352   0.006111\n",
      "12      1stFlrSF     -0.784149  190830.828029     0.030515   0.007082\n",
      "13      2ndFlrSF     -0.890109  217015.007731     0.036335   0.006053\n",
      "14  LowQualFinSF     -0.089288   21382.374576     0.001994   0.004639\n",
      "15     GrLivArea      1.156969  258555.961950     0.053184   0.006372\n",
      "16  BsmtFullBath      0.020035       0.006588     0.020035   0.006588\n",
      "17  BsmtHalfBath     -0.000388       0.004732    -0.000388   0.004732\n",
      "18      FullBath      0.016777       0.007517     0.016777   0.007517\n",
      "19      HalfBath      0.011940       0.006582     0.011940   0.006582\n",
      "\n",
      "^2 (solve) = 0.019542817159904243    ^2 (pinv) = 0.01954281715990427\n",
      "max |beta difference| = 3.6465843374445948\n"
     ]
    }
   ],
   "source": [
    "# pseudo inverse matrices\n",
    "A_pinv    = np.linalg.pinv(A)            # A^+\n",
    "ATA_pinv  = np.linalg.pinv(A.T @ A)      # (A^T A)^+\n",
    "\n",
    "#estimation of coefficients via pseudoinverse\n",
    "beta_pinv = A_pinv @ y_vec\n",
    "\n",
    "# Fitted values and residuals\n",
    "yhat_pinv = A @ beta_pinv\n",
    "res_pinv  = y_vec - yhat_pinv\n",
    "\n",
    "# Residual variance (same df as in (ii): m - (d+1))\n",
    "sigma2_pinv = (res_pinv @ res_pinv) / (m - (d + 1))\n",
    "\n",
    "# Var-cov and standard errors using (A^T A)^+\n",
    "se_beta_pinv  = np.sqrt(np.diag(sigma2_pinv * ATA_pinv))\n",
    "\n",
    "# Comparison table (normal-equation vs pseudoinverse)\n",
    "cmp = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"beta (solve)\": beta_hat,\n",
    "    \"SE (solve)\":   se_beta,\n",
    "    \"beta (pinv)\":  beta_pinv,\n",
    "    \"SE (pinv)\":    se_beta_pinv\n",
    "})\n",
    "print(cmp.head(20))\n",
    "\n",
    "print(\"\\n^2 (solve) =\", sigma2_hat, \"   ^2 (pinv) =\", sigma2_pinv)\n",
    "print(\"max |beta difference| =\", np.max(np.abs(beta_hat - beta_pinv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd7854d",
   "metadata": {},
   "source": [
    "We replaced the normal equations solution $(A^\\top A)^{-1} A^\\top y$ by the MoorePenrose pseudoinverse $A^+ y$ using np.linalg.pinv. This approach is numerically stable and produces valid coefficient estimates even when the design matrix is not full rank. We then recomputed coefficients, residual variance, and standard errors using the pseudoinverse.\n",
    "\n",
    "The results show that for most predictors, the coefficients and standard errors are identical between the normal-equation and pseudoinverse solutions. However, for the basement square-footage variables, which were collinear (`TotalBsmtSF` vs. its components), the pseudoinverse provides finite coefficients and SEs, while the normal equations failed and returned NaN. The residual variance remained the same in both methods ($\\hat\\sigma^2 = 0.0195$), since overall fit does not change.\n",
    "\n",
    "The pseudoinverse gives the minimum-norm solution in collinear settings, which explains why basement features now have different, smaller coefficients. When the matrix is full rank, both methods are identical; when collinearity is present, only the pseudoinverse can still return a defined solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba2693b",
   "metadata": {},
   "source": [
    "#### (v)  \n",
    "Confirm your results using the OLS function from the `statsmodels` package. Report the coefficient table and standard errors, and check that they match your matrix-algebra results up to numerical rounding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99944aef",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1114545-6d94-4276-9e6a-cc84a49309bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Feature  Coef (statsmodels)  StdErr (statsmodels)  Coef (numpy)   \n",
      "0       Intercept           12.025544              0.004368     12.025544  \\\n",
      "1     LotFrontage            0.019239              0.005133      0.019239   \n",
      "2         LotArea            0.023077              0.004945      0.023077   \n",
      "3     OverallQual            0.106961              0.007807      0.106961   \n",
      "4     OverallCond            0.057950              0.005622      0.057950   \n",
      "5       YearBuilt            0.088736              0.009800      0.088736   \n",
      "6    YearRemodAdd            0.021237              0.007067      0.021237   \n",
      "7      MasVnrArea           -0.002847              0.005112     -0.002847   \n",
      "8      BsmtFinSF1            0.034486              0.004933     -3.603264   \n",
      "9      BsmtFinSF2            0.002185              0.004492     -1.329099   \n",
      "10      BsmtUnfSF            0.007698              0.004112     -3.638886   \n",
      "11    TotalBsmtSF            0.044352              0.006105      3.571429   \n",
      "12       1stFlrSF            0.030515              0.007074     -0.784149   \n",
      "13       2ndFlrSF            0.036335              0.006047     -0.890109   \n",
      "14   LowQualFinSF            0.001994              0.004634     -0.089288   \n",
      "15      GrLivArea            0.053184              0.006366      1.156969   \n",
      "16   BsmtFullBath            0.020035              0.006582      0.020035   \n",
      "17   BsmtHalfBath           -0.000388              0.004727     -0.000388   \n",
      "18       FullBath            0.016777              0.007510      0.016777   \n",
      "19       HalfBath            0.011940              0.006576      0.011940   \n",
      "20   BedroomAbvGr           -0.000443              0.006691     -0.000443   \n",
      "21   KitchenAbvGr           -0.015755              0.005076     -0.015755   \n",
      "22   TotRmsAbvGrd            0.021091              0.009510      0.021091   \n",
      "23     Fireplaces            0.027771              0.005454      0.027771   \n",
      "24    GarageYrBlt           -0.002502              0.008126     -0.002502   \n",
      "25     GarageCars            0.027982              0.010497      0.027982   \n",
      "26     GarageArea            0.024236              0.010205      0.024236   \n",
      "27     WoodDeckSF            0.009027              0.004787      0.009027   \n",
      "28    OpenPorchSF           -0.004673              0.004901     -0.004673   \n",
      "29  EnclosedPorch            0.008867              0.004889      0.008867   \n",
      "30      3SsnPorch            0.006771              0.004434      0.006771   \n",
      "31    ScreenPorch            0.021770              0.004659      0.021770   \n",
      "32       PoolArea           -0.000684              0.004505     -0.000684   \n",
      "33        MiscVal           -0.000391              0.004420     -0.000391   \n",
      "34         MoSold           -0.005261              0.004490     -0.005261   \n",
      "35         YrSold           -0.013225              0.004483     -0.013225   \n",
      "\n",
      "    StdErr (numpy)      | coef|         | SE|  \n",
      "0         0.004373  1.421085e-14       0.000004  \n",
      "1         0.005138  1.273287e-15       0.000005  \n",
      "2         0.004950  2.949030e-16       0.000005  \n",
      "3         0.007815  5.828671e-16       0.000008  \n",
      "4         0.005628  1.373901e-15       0.000006  \n",
      "5         0.009810  1.526557e-16       0.000010  \n",
      "6         0.007075  2.636780e-15       0.000007  \n",
      "7         0.005117  3.132477e-15       0.000005  \n",
      "8    676653.114937  3.637750e+00  676653.110004  \n",
      "9    251590.780836  1.331284e+00  251590.776344  \n",
      "10   686998.521620  3.646584e+00  686998.517508  \n",
      "11   675493.031813  3.527077e+00  675493.025708  \n",
      "12   190830.828029  8.146636e-01  190830.820955  \n",
      "13   217015.007731  9.264448e-01  217015.001685  \n",
      "14    21382.374576  9.128212e-02   21382.369941  \n",
      "15   258555.961950  1.103785e+00  258555.955584  \n",
      "16        0.006588  6.245005e-16       0.000007  \n",
      "17        0.004732  8.450272e-16       0.000005  \n",
      "18        0.007517  6.279699e-16       0.000008  \n",
      "19        0.006582  1.661865e-15       0.000007  \n",
      "20        0.006698  1.692385e-15       0.000007  \n",
      "21        0.005081  8.708312e-16       0.000005  \n",
      "22        0.009520  7.771561e-16       0.000010  \n",
      "23        0.005459  1.804112e-15       0.000006  \n",
      "24        0.008134  2.282462e-15       0.000008  \n",
      "25        0.010507  8.285039e-15       0.000011  \n",
      "26        0.010215  6.834810e-15       0.000010  \n",
      "27        0.004792  1.752071e-16       0.000005  \n",
      "28        0.004906  4.085274e-16       0.000005  \n",
      "29        0.004894  2.619432e-16       0.000005  \n",
      "30        0.004439  1.989728e-15       0.000004  \n",
      "31        0.004664  3.899658e-15       0.000005  \n",
      "32        0.004510  5.275728e-16       0.000005  \n",
      "33        0.004425  1.181238e-16       0.000004  \n",
      "34        0.004494  1.273287e-15       0.000005  \n",
      "35        0.004487  1.942890e-16       0.000005  \n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              SalePrice   R-squared:                       0.886\n",
      "Model:                            OLS   Adj. R-squared:                  0.882\n",
      "Method:                 Least Squares   F-statistic:                     232.1\n",
      "Date:                Sun, 12 Oct 2025   Prob (F-statistic):               0.00\n",
      "Time:                        09:26:42   Log-Likelihood:                 579.03\n",
      "No. Observations:                1022   AIC:                            -1090.\n",
      "Df Residuals:                     988   BIC:                            -922.5\n",
      "Df Model:                          33                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================\n",
      "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "const            12.0255      0.004   2752.812      0.000      12.017      12.034\n",
      "LotFrontage       0.0192      0.005      3.748      0.000       0.009       0.029\n",
      "LotArea           0.0231      0.005      4.666      0.000       0.013       0.033\n",
      "OverallQual       0.1070      0.008     13.701      0.000       0.092       0.122\n",
      "OverallCond       0.0580      0.006     10.307      0.000       0.047       0.069\n",
      "YearBuilt         0.0887      0.010      9.054      0.000       0.070       0.108\n",
      "YearRemodAdd      0.0212      0.007      3.005      0.003       0.007       0.035\n",
      "MasVnrArea       -0.0028      0.005     -0.557      0.578      -0.013       0.007\n",
      "BsmtFinSF1        0.0345      0.005      6.991      0.000       0.025       0.044\n",
      "BsmtFinSF2        0.0022      0.004      0.486      0.627      -0.007       0.011\n",
      "BsmtUnfSF         0.0077      0.004      1.872      0.061      -0.000       0.016\n",
      "TotalBsmtSF       0.0444      0.006      7.265      0.000       0.032       0.056\n",
      "1stFlrSF          0.0305      0.007      4.313      0.000       0.017       0.044\n",
      "2ndFlrSF          0.0363      0.006      6.009      0.000       0.024       0.048\n",
      "LowQualFinSF      0.0020      0.005      0.430      0.667      -0.007       0.011\n",
      "GrLivArea         0.0532      0.006      8.355      0.000       0.041       0.066\n",
      "BsmtFullBath      0.0200      0.007      3.044      0.002       0.007       0.033\n",
      "BsmtHalfBath     -0.0004      0.005     -0.082      0.935      -0.010       0.009\n",
      "FullBath          0.0168      0.008      2.234      0.026       0.002       0.032\n",
      "HalfBath          0.0119      0.007      1.816      0.070      -0.001       0.025\n",
      "BedroomAbvGr     -0.0004      0.007     -0.066      0.947      -0.014       0.013\n",
      "KitchenAbvGr     -0.0158      0.005     -3.104      0.002      -0.026      -0.006\n",
      "TotRmsAbvGrd      0.0211      0.010      2.218      0.027       0.002       0.040\n",
      "Fireplaces        0.0278      0.005      5.092      0.000       0.017       0.038\n",
      "GarageYrBlt      -0.0025      0.008     -0.308      0.758      -0.018       0.013\n",
      "GarageCars        0.0280      0.010      2.666      0.008       0.007       0.049\n",
      "GarageArea        0.0242      0.010      2.375      0.018       0.004       0.044\n",
      "WoodDeckSF        0.0090      0.005      1.886      0.060      -0.000       0.018\n",
      "OpenPorchSF      -0.0047      0.005     -0.953      0.341      -0.014       0.005\n",
      "EnclosedPorch     0.0089      0.005      1.813      0.070      -0.001       0.018\n",
      "3SsnPorch         0.0068      0.004      1.527      0.127      -0.002       0.015\n",
      "ScreenPorch       0.0218      0.005      4.673      0.000       0.013       0.031\n",
      "PoolArea         -0.0007      0.005     -0.152      0.879      -0.010       0.008\n",
      "MiscVal          -0.0004      0.004     -0.088      0.930      -0.009       0.008\n",
      "MoSold           -0.0053      0.004     -1.172      0.242      -0.014       0.004\n",
      "YrSold           -0.0132      0.004     -2.950      0.003      -0.022      -0.004\n",
      "==============================================================================\n",
      "Omnibus:                      626.172   Durbin-Watson:                   1.960\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            18509.147\n",
      "Skew:                          -2.289   Prob(JB):                         0.00\n",
      "Kurtosis:                      23.340   Cond. No.                     6.00e+15\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 2.04e-28. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "# add explicit intercept column\n",
    "X_sm = sm.add_constant(X_train[num_cols], has_constant='add')  # const first\n",
    "ols = sm.OLS(y_train, X_sm).fit()\n",
    "\n",
    "# Coefficients and standard errors from statsmodels\n",
    "sm_table = pd.DataFrame({\n",
    "    \"Feature\": ols.params.index,              \n",
    "    \"Coef (statsmodels)\": ols.params.values,\n",
    "    \"StdErr (statsmodels)\": ols.bse.values\n",
    "})\n",
    "\n",
    "#'Intercept' vs statsmodels 'const'\n",
    "sm_table[\"Feature\"] = sm_table[\"Feature\"].replace({\"const\": \"Intercept\"})\n",
    "\n",
    "#matrix-algebra results\n",
    "np_coefs = pd.Series(beta_hat, index=features, name=\"Coef (numpy)\")\n",
    "np_ses   = pd.Series(se_beta,  index=features, name=\"StdErr (numpy)\")\n",
    "\n",
    "cmp = (\n",
    "    sm_table\n",
    "    .merge(np_coefs, left_on=\"Feature\", right_index=True, how=\"left\")\n",
    "    .merge(np_ses,   left_on=\"Feature\", right_index=True, how=\"left\")\n",
    ")\n",
    "\n",
    "# Absolute differences for checking\n",
    "cmp[\"| coef|\"] = (cmp[\"Coef (statsmodels)\"] - cmp[\"Coef (numpy)\"]).abs()\n",
    "cmp[\"| SE|\"]   = (cmp[\"StdErr (statsmodels)\"] - cmp[\"StdErr (numpy)\"]).abs()\n",
    "\n",
    "print(cmp)\n",
    "print(ols.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f18a0",
   "metadata": {},
   "source": [
    "We confirmed our results by fitting the same model with the OLS implementation in statsmodels, which automatically provides coefficients and robust standard errors. We then compared these outputs to our earlier matrix-algebra results.\n",
    "\n",
    "For most predictors, the coefficients and standard errors matched exactly (up to numerical rounding), confirming the correctness of our manual implementation. However, for the basement square-footage variables and the living-area components, statsmodels returned finite estimates, while the normal equations gave unstable or NaN values. This difference arises because statsmodels internally uses the pseudoinverse, which can still produce a defined solution even when the design matrix is nearly singular.\n",
    "\n",
    "The residual variance estimate and fit quality remained unchanged, but the model summary from statsmodels explicitly flagged the issue: the smallest eigenvalue of the design matrix is extremely close to zero, confirming the severe multicollinearity we had already diagnosed. This exercise shows that the matrix-algebra derivations and statsmodels are consistent when the design matrix is full rank, but differ in collinear settings where only the pseudoinverse solution is well-defined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31c5ecc-7f64-4788-9237-76dff4e3fb3e",
   "metadata": {},
   "source": [
    "### Question 3 - Regularization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1ec5f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d209e93",
   "metadata": {},
   "source": [
    "#### 3. \n",
    "In this question, you will implement regularization techniques and compare their performance to ordinary least squares. Work with the full *Housing* dataset prepared in Question 1, which already includes both numerical features (standardized) and categorical features (one-hot encoded dummies). Use the same training and test splits as before to ensure consistency across questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcd0948",
   "metadata": {},
   "source": [
    "**a)** Fit an OLS regression of the (potentially transformed) target variable *SalePrice* on all explanatory variables in the prepared *Housing* dataset. Report the in-sample and out-of-sample MSE and $R^2$. Compare these results to the numerical-only regression from Question 2.a). How do the in-sample and out-of-sample metrics change when including categorical features, and what does this reveal about the models ability to generalize?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e56b5fb",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e1a4baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset performance on log(SalePrice):\n",
      "In-sample MSE: 0.0084, R: 0.9489\n",
      "Out-of-sample MSE: 0.1078, R: 0.2641\n",
      "\n",
      "Full dataset performance on original SalePrice scale:\n",
      "In-sample MSE: 299387281, R: 0.9554\n",
      "Out-of-sample MSE: 5967565261, R: -0.1145\n",
      "Full rank: 263, Real rank: 253\n"
     ]
    }
   ],
   "source": [
    "# Fit linear regression on the full preprocessed dataset (X_train_enc, X_test_enc)\n",
    "#linreg_full = LinearRegression(fit_intercept=True)\n",
    "#linreg_full.fit(X_train_enc, y_train)\n",
    "X_sm = sm.add_constant(X_train_enc.astype('float32'), has_constant='add')  # const first\n",
    "X_test_sm = sm.add_constant(X_test_enc.astype('float32'), has_constant='add')\n",
    "                    \n",
    "linreg_full = sm.OLS(y_train, X_sm).fit()\n",
    "# Make predictions\n",
    "y_train_pred_full = linreg_full.predict(X_sm)\n",
    "\n",
    "y_test_pred_full = linreg_full.predict(X_test_sm)\n",
    "\n",
    "# Calculate performance metrics on log scale\n",
    "mse_train_full_log = mean_squared_error(y_train, y_train_pred_full)\n",
    "r2_train_full_log = r2_score(y_train, y_train_pred_full)\n",
    "mse_test_full_log = mean_squared_error(y_test, y_test_pred_full)\n",
    "r2_test_full_log = r2_score(y_test, y_test_pred_full)\n",
    "\n",
    "print(f\"Full dataset performance on log(SalePrice):\")\n",
    "print(f\"In-sample MSE: {mse_train_full_log:.4f}, R: {r2_train_full_log:.4f}\")\n",
    "print(f\"Out-of-sample MSE: {mse_test_full_log:.4f}, R: {r2_test_full_log:.4f}\")\n",
    "\n",
    "# Inverse-transform predictions to original scale\n",
    "y_train_pred_full_orig = np.exp(y_train_pred_full)\n",
    "y_test_pred_full_orig = np.exp(y_test_pred_full)\n",
    "y_train_orig = np.exp(y_train)\n",
    "y_test_orig = np.exp(y_test)\n",
    "\n",
    "# calculate performance metrics on original scale \n",
    "mse_train_full_orig = mean_squared_error(y_train_orig, y_train_pred_full_orig)\n",
    "r2_train_full_orig = r2_score(y_train_orig, y_train_pred_full_orig)\n",
    "mse_test_full_orig = mean_squared_error(y_test_orig, y_test_pred_full_orig)\n",
    "r2_test_full_orig = r2_score(y_test_orig, y_test_pred_full_orig)\n",
    "\n",
    "print(f\"\\nFull dataset performance on original SalePrice scale:\")\n",
    "print(f\"In-sample MSE: {mse_train_full_orig:.0f}, R: {r2_train_full_orig:.4f}\")\n",
    "print(f\"Out-of-sample MSE: {mse_test_full_orig:.0f}, R: {r2_test_full_orig:.4f}\")\n",
    "\n",
    "# Check for singularity because of wierd results\n",
    "X_train_float = X_train_enc.astype('float64')\n",
    "X_test_float = X_test_enc.astype('float64')\n",
    "\n",
    "# rank deficiency\n",
    "full_rank = min(X_train_float.shape)\n",
    "real_rank = np.linalg.matrix_rank(X_train_float.values)\n",
    "print(f\"Full rank: {full_rank}, Real rank: {real_rank}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a3ffeb",
   "metadata": {},
   "source": [
    "When fitting OLS regression on the full Housing dataset (including both numerical and categorical features), the model achieved strong in-sample performance with R = 0.9489 and MSE = 0.0084 on the log scale, compared to the numerical-only model from Question 2.a which had R = 0.8857 and MSE = 0.0189. This improvement demonstrates that categorical features (one-hot encoded) contain valuable information for predicting house prices. However, the out-of-sample performance tells a different story: R = 0.2641 and MSE = 0.1078 on log scale, indicating substantial overfitting. The model fits training data very well but generalizes poorly to unseen data.\n",
    "\n",
    "The rank deficiency analysis reveals the underlying problem: the design matrix has real rank 253 despite having 263 features, meaning 10 features are redundant linear combinations of others. This multicollinearity causes numerical instability in coefficient estimation and contributes to poor generalization. When predictions are inverse-transformed to the original SalePrice scale, performance deteriorates further with out-of-sample R = -0.1146, meaning the model performs worse than simply predicting the mean price. This occurs because the exponential transformation amplifies prediction errors, particularly for expensive houses where small log-scale mistakes translate to large dollar-amount errors.\n",
    "\n",
    "Comparing to the numerical-only model (which had out-of-sample R = 0.7735 on log scale), adding categorical features actually hurt generalization despite improving training fit. The increased model complexity from one-hot encoding introduced overfitting that outweighed the additional predictive information. This demonstrates the bias-variance tradeoff: while categorical features reduce bias by capturing more patterns, they substantially increase variance when the model has too many parameters relative to the number of training observations. The appropriate comparison should focus on log-scale metrics since the model was trained on log-transformed targets, where we see the full model overfits (R drops from 0.95 to 0.26) while the simpler numerical model generalizes better (R of 0.77 out-of-sample)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9365d348",
   "metadata": {},
   "source": [
    "**b)** Implement the Truncated Pseudoinverse, Ridge, Lasso, and Elastic Net regressions. Use 8-fold cross-validation on the training set to tune the hyperparameters of each regularization technique, using the MSE as the selection criterion. Compare their in-sample and out-of-sample performance (MSE and R^2) with the OLS results from Questions 2.a) and 3.a). All regressions should include an intercept term. The intercept must not be penalized during regularization. Why is it important that the intercept is not penalized in these models?\n",
    "\n",
    "#### try with a relative percentage of singular values excluded and not absolute value as measure because different folds have different singular values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440ea6cc",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a019674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Threshold  8-fold CV error (MSE-based)\n",
      "0         0.376494                     0.032853\n",
      "1         0.162975                     0.033291\n",
      "2         0.123285                     0.033291\n",
      "3         0.284804                     0.033696\n",
      "4         0.000001                     0.034010\n",
      "..             ...                          ...\n",
      "95      305.385551                   144.777998\n",
      "96      231.012970                   144.777998\n",
      "97      174.752840                   144.777998\n",
      "98     8697.490026                   144.777998\n",
      "99  1000000.000000                   144.777998\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "Best threshold: 0.37649358067924715\n"
     ]
    }
   ],
   "source": [
    "# Truncated Pseudeinverse \n",
    "\n",
    "#Include the intercept \n",
    "X_train_with_int = X_train_float.copy()\n",
    "X_test_with_int = X_test_float.copy()\n",
    "X_train_with_int.insert(0, \"Intercept\", 1)\n",
    "X_test_with_int.insert(0, \"Intercept\", 1)\n",
    "\n",
    "\n",
    "# Ensure the test set has the same columns as the train set (reindex columns)\n",
    "X_test_with_int = X_test_with_int.reindex(columns=X_train_with_int.columns, fill_value=0)\n",
    "\n",
    "# 8 folcd cross-validation\n",
    "cv = KFold(n_splits=8, shuffle=True, random_state=3)\n",
    "# threshold for singular values\n",
    "steps = np.logspace(-6, 6, 100)\n",
    "mse_errors_cross_validation_trpseuinv = np.zeros(len(steps))\n",
    "\n",
    "# loop through folds\n",
    "for train_index, test_index in cv.split(X_train_with_int, y_train):\n",
    "   # first we train on the K-1 folds and decide which indexes are in train and test set\n",
    "   Xtr = X_train_with_int.iloc[train_index, :].values\n",
    "   Ytr = y_train.iloc[train_index].values\n",
    "   Xte = X_train_with_int.iloc[test_index, :].values\n",
    "   Yte = y_train.iloc[test_index].values\n",
    "   # SVD decomposition\n",
    "   P, D, Q = np.linalg.svd(Xtr, full_matrices=False)\n",
    "   #loop through thresholds and saving the mse for later cross validation\n",
    "   for ind, threshold in enumerate(steps):  # CHANGE 2: Fixed typo \"treshold\"\n",
    "      k = int(np.sum(D > threshold))\n",
    "      if k == 0:\n",
    "            y_hat = np.zeros_like(Yte)\n",
    "      else:\n",
    "          Qk = Q.T[:, :k]\n",
    "          D_inv_k = np.diag(1.0 / D[:k]) \n",
    "          Pk_T = P.T[:k, :]\n",
    "          X_trunc_pseuinv = Qk @ D_inv_k @ Pk_T\n",
    "          betas_c = X_trunc_pseuinv @ Ytr\n",
    "          y_hat   = Xte @ betas_c\n",
    "      #store mse errors\n",
    "      mse = np.mean((Yte - y_hat) ** 2)\n",
    "      mse_errors_cross_validation_trpseuinv[ind] += mse \n",
    "\n",
    "mse_errors_cross_validation_trpseuinv /= cv.get_n_splits()\n",
    "\n",
    "# Store cross-validation results in a df         \n",
    "results = (\n",
    "    pd.DataFrame({\n",
    "        \"Threshold\": steps,\n",
    "        \"8-fold CV error (MSE-based)\": mse_errors_cross_validation_trpseuinv\n",
    "    })\n",
    "    .sort_values(\"8-fold CV error (MSE-based)\", ascending=True, ignore_index=True)\n",
    ")\n",
    "print(results)\n",
    "\n",
    "best_idx = np.argmin(mse_errors_cross_validation_trpseuinv)\n",
    "trunc_pseuinv_c = steps[best_idx]\n",
    "print(f\"Best threshold: {trunc_pseuinv_c}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1add6556",
   "metadata": {},
   "source": [
    "Based on the 8-fold cross-validation results for the truncated pseudoinverse, we chose the threshold value approx.\n",
    "$c = 0.37649$. This threshold gave the lowest validation MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b50381af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_pseuinv_c = steps[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cec39e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen c = 0.376494  |  kept components k = 244 of 264\n",
      "\n",
      "(Truncated pseudoinverse - performance on log scale\n",
      " Train: MSE = 0.010104   R^2 = 0.9388\n",
      " Test : MSE = 1.368700   R^2 = -8.3463\n",
      "\n",
      "Truncated pseudoinverse - performance on original scale:\n",
      " Train: MSE = 389479307   R^2 = 0.9420\n",
      " Test : MSE = 17873593885   R^2 = -2.3382\n"
     ]
    }
   ],
   "source": [
    "# convert to numpy because of multiplying\n",
    "Xtr_np = X_train_with_int.to_numpy(dtype=float)   \n",
    "Xte_np = X_test_with_int.to_numpy(dtype=float)    \n",
    "y_tr_np = np.asarray(y_train, dtype=float).ravel()\n",
    "y_te_np = np.asarray(y_test,  dtype=float).ravel()\n",
    "\n",
    "# SVD decomposition\n",
    "P, D, Q = np.linalg.svd(Xtr_np, full_matrices=False)\n",
    "\n",
    "# element where we stop \n",
    "k = int(np.sum(D > trunc_pseuinv_c))\n",
    "print(f\"Chosen c = {trunc_pseuinv_c:.6g}  |  kept components k = {k} of {len(D)}\")\n",
    "\n",
    "# composite the truncated pseudoinverse\n",
    "Qk = Q.T[:, :k]\n",
    "D_inv_k = np.diag(1.0 / D[:k]) \n",
    "Pk_T = P.T[:k, :]\n",
    "X_trunc_pseuinv = Qk @ D_inv_k @ Pk_T\n",
    "\n",
    "# fit coeficients on training data\n",
    "betas_c = X_trunc_pseuinv @ y_tr_np\n",
    "\n",
    "# predicitons\n",
    "y_hat_tr = Xtr_np @ betas_c\n",
    "y_hat_te = Xte_np @ betas_c\n",
    "\n",
    "# original scale \n",
    "y_train_orig_tp = np.exp(y_tr_np)\n",
    "y_test_orig_tp  = np.exp(y_te_np)\n",
    "y_hat_tr_orig = np.exp(y_hat_tr)\n",
    "y_hat_te_orig = np.exp(y_hat_te)\n",
    "\n",
    "#performance metrics on log and original scale\n",
    "mse_tr_log = mean_squared_error(y_tr_np, y_hat_tr)\n",
    "r2_tr_log  = r2_score(y_tr_np, y_hat_tr)\n",
    "mse_te_log = mean_squared_error(y_te_np, y_hat_te)\n",
    "r2_te_log  = r2_score(y_te_np, y_hat_te)\n",
    "\n",
    "print(\"\\n(Truncated pseudoinverse - performance on log scale\")\n",
    "print(f\" Train: MSE = {mse_tr_log:.6f}   R^2 = {r2_tr_log:.4f}\")\n",
    "print(f\" Test : MSE = {mse_te_log:.6f}   R^2 = {r2_te_log:.4f}\")\n",
    "\n",
    "mse_tr_orig = mean_squared_error(y_train_orig_tp, y_hat_tr_orig)\n",
    "r2_tr_orig  = r2_score(y_train_orig_tp, y_hat_tr_orig)\n",
    "mse_te_orig = mean_squared_error(y_test_orig_tp, y_hat_te_orig)\n",
    "r2_te_orig  = r2_score(y_test_orig_tp, y_hat_te_orig)\n",
    "\n",
    "print(\"\\nTruncated pseudoinverse - performance on original scale:\")\n",
    "print(f\" Train: MSE = {mse_tr_orig:.0f}   R^2 = {r2_tr_orig:.4f}\")\n",
    "print(f\" Test : MSE = {mse_te_orig:.0f}   R^2 = {r2_te_orig:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72308551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Alpha  8-fold CV error (MSE-based)\n",
      "0        24.770764                     0.017197\n",
      "1        18.738174                     0.017201\n",
      "2        32.745492                     0.017244\n",
      "3        14.174742                     0.017256\n",
      "4        43.287613                     0.017341\n",
      "..             ...                          ...\n",
      "95   327454.916288                     0.159045\n",
      "96   432876.128108                     0.160484\n",
      "97   572236.765935                     0.161589\n",
      "98   756463.327555                     0.162433\n",
      "99  1000000.000000                     0.163078\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "24.77076355991714\n",
      "Train MSE: 0.012331375237722314  R^2: 0.9252737268536805\n",
      "Test  MSE: 0.031227470341743124  R^2: 0.7867601318124374\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Ridge \n",
    "\n",
    "# Take datasets without intercept from before\n",
    "X_train_enc_np = X_train_enc.to_numpy(dtype=float)\n",
    "X_test_enc_np = X_test_enc.to_numpy(dtype=float)\n",
    "\n",
    "# lambdas\n",
    "steps = np.logspace(-6, 6, 100)\n",
    "\n",
    "# Ridge\n",
    "ridge = Ridge(fit_intercept=True)\n",
    "gridsearch_ridge = GridSearchCV(ridge, {'alpha': steps}, scoring='neg_mean_squared_error', cv=8)\n",
    "gridsearch_ridge.fit(X_train_enc_np, y_train)  # CHANGE 7: Use _np version\n",
    "\n",
    "# Dataframe with results MSE\n",
    "cv_df = pd.DataFrame({\n",
    "    \"Alpha\": gridsearch_ridge.cv_results_[\"param_alpha\"].data.astype(float),\n",
    "    \"8-fold CV error (MSE-based)\": -gridsearch_ridge.cv_results_[\"mean_test_score\"]\n",
    "}).sort_values(\"8-fold CV error (MSE-based)\", ascending=True, ignore_index=True)\n",
    "\n",
    "print(cv_df)\n",
    "\n",
    "# best lambda/alpha\n",
    "best_idx = gridsearch_ridge.best_index_\n",
    "print(steps[best_idx])\n",
    "\n",
    "#quick check\n",
    "yhat_tr = gridsearch_ridge.best_estimator_.predict(X_train_enc_np)\n",
    "yhat_te = gridsearch_ridge.best_estimator_.predict(X_test_enc_np)\n",
    "\n",
    "print(\"Train MSE:\", mean_squared_error(y_train, yhat_tr), \" R^2:\", r2_score(y_train, yhat_tr))\n",
    "print(\"Test  MSE:\", mean_squared_error(y_test, yhat_te),  \" R^2:\", r2_score(y_test, yhat_te))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03dbd1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.117e+00, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.388e+00, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.801e+00, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.215e+00, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.678e+00, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.941e+00, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.926e+00, tolerance: 1.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.888e+00, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.593e+00, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.078e+00, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.354e+00, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.995e-01, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.361e+00, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.578e+00, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.680e+00, tolerance: 1.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.579e+00, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.812e-01, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.693e+00, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.015e-01, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.286e-01, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.052e+00, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.225e+00, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.363e+00, tolerance: 1.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.286e+00, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.678e-01, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.233e+00, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.186e-01, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.904e-01, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.687e-01, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.597e-01, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.969e+00, tolerance: 1.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e-01, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.365e-01, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.195e-01, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.873e-02, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.851e-01, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.227e-01, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.273e-01, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.492e+00, tolerance: 1.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.333e-01, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.635e-02, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.305e-01, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.892e-02, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.255e-01, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.397e-01, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.599e-01, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.222e-01, tolerance: 1.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.784e-01, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.314e-01, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.597e-02, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.005e-02, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.030e-02, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.019e-01, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.118e-02, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.464e-01, tolerance: 1.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.528e-01, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.902e-01, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.031e-02, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.628e-02, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.275e-01, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.569e-02, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.581e-01, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.800e-02, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.261e-02, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.502e-02, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.305e-02, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.267e-02, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.384e-02, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.276e-02, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.374e-02, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.579e-02, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.636e-02, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.510e-02, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.774e-02, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.791e-02, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.097e-02, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.477e-02, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.067e-02, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.709e-02, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.043e-02, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.523e-02, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.617e-02, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Alpha  8-fold CV error (MSE-based)\n",
      "0         0.001072                     0.017808\n",
      "1         0.001417                     0.017899\n",
      "2         0.000811                     0.017950\n",
      "3         0.000614                     0.018300\n",
      "4         0.001874                     0.018389\n",
      "..             ...                          ...\n",
      "95      132.194115                     0.165109\n",
      "96      174.752840                     0.165109\n",
      "97      231.012970                     0.165109\n",
      "98       10.722672                     0.165109\n",
      "99  1000000.000000                     0.165109\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "0.0010722672220103231\n",
      "Train MSE: 0.013240056860550945  R^2: 0.9197672533386408\n",
      "Test  MSE: 0.028543665315635156  R^2: 0.8050867597379587\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "\n",
    "# Lasso model intercept is not penalized\n",
    "lasso = Lasso(fit_intercept=True)\n",
    "\n",
    "# Grid search with 8-fold CV on MSE\n",
    "gridsearch_lasso = GridSearchCV(lasso, {'alpha': steps}, scoring='neg_mean_squared_error', cv=8)\n",
    "\n",
    "gridsearch_lasso.fit(X_train_enc_np, y_train)\n",
    "\n",
    "# Dataframe with results MSE\n",
    "cv_df_lasso = (\n",
    "    pd.DataFrame({\n",
    "        \"Alpha\": gridsearch_lasso.cv_results_[\"param_alpha\"].data.astype(float),\n",
    "        \"8-fold CV error (MSE-based)\": -gridsearch_lasso.cv_results_[\"mean_test_score\"]\n",
    "    })\n",
    "    .sort_values(\"8-fold CV error (MSE-based)\", ascending=True, ignore_index=True)\n",
    ")\n",
    "print(cv_df_lasso)\n",
    "\n",
    "# Best lambda/alpha\n",
    "best_idx_lasso = gridsearch_lasso.best_index_\n",
    "print(steps[best_idx_lasso])\n",
    "\n",
    "# Quick train/test check\n",
    "yhat_tr_lasso = gridsearch_lasso.best_estimator_.predict(X_train_enc_np)\n",
    "yhat_te_lasso = gridsearch_lasso.best_estimator_.predict(X_test_enc_np)\n",
    "\n",
    "print(\"Train MSE:\", mean_squared_error(y_train, yhat_tr_lasso),\n",
    "      \" R^2:\", r2_score(y_train, yhat_tr_lasso))\n",
    "print(\"Test  MSE:\", mean_squared_error(y_test,  yhat_te_lasso),\n",
    "      \" R^2:\", r2_score(y_test,  yhat_te_lasso))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6163ef9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.600e+00, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.451e+00, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.238e+00, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.262e+00, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.581e+00, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.719e+00, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.729e+00, tolerance: 1.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.296e+00, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.434e+00, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.317e+00, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.071e+00, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.854e+00, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.251e+00, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.457e+00, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.632e+00, tolerance: 1.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.116e+00, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.270e+00, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.188e+00, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.905e+00, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.532e+00, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.964e+00, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.218e+00, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.537e+00, tolerance: 1.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.942e+00, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.106e+00, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.064e+00, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.743e+00, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.250e+00, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.712e+00, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.997e+00, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.444e+00, tolerance: 1.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.772e+00, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.944e+00, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.944e+00, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.581e+00, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.008e+00, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.490e+00, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.791e+00, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.354e+00, tolerance: 1.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.607e+00, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.782e+00, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.827e+00, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.423e+00, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.801e+00, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.292e+00, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.601e+00, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.266e+00, tolerance: 1.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.443e+00, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.618e+00, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.715e+00, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.268e+00, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.621e+00, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.121e+00, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.425e+00, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.180e+00, tolerance: 1.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.288e+00, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.455e+00, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.605e+00, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.114e+00, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.467e+00, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.964e+00, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.263e+00, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.096e+00, tolerance: 1.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.150e+00, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.287e+00, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.500e+00, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.963e+00, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.328e+00, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.823e+00, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.113e+00, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.014e+00, tolerance: 1.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.029e+00, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.131e+00, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.397e+00, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.814e+00, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.225e+00, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.689e+00, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.953e+00, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.934e+00, tolerance: 1.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.899e+00, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.331e+00, tolerance: 1.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.258e+00, tolerance: 1.512e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.477e+00, tolerance: 1.455e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.027e+00, tolerance: 1.442e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.121e+00, tolerance: 1.419e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.412e+00, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.535e+00, tolerance: 1.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.106e+00, tolerance: 1.507e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.027e-02, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.235e-02, tolerance: 1.448e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Alpha  l1_ratio  8-fold CV error (MSE-based)\n",
      "0         0.001000  0.770222                     0.017751\n",
      "1         0.001000  0.660333                     0.017775\n",
      "2         0.001000  0.880111                     0.017777\n",
      "3         0.001000  0.990000                     0.017826\n",
      "4         0.001000  0.550444                     0.017851\n",
      "5         0.001000  0.440556                     0.017911\n",
      "6         0.001000  0.330667                     0.017943\n",
      "7         0.001000  0.220778                     0.018101\n",
      "8         0.001000  0.110889                     0.018550\n",
      "9         0.001000  0.001000                     0.019448\n",
      "10        1.000000  0.001000                     0.023499\n",
      "11        0.000001  0.440556                     0.026449\n",
      "12        0.000001  0.550444                     0.026451\n",
      "13        0.000001  0.330667                     0.026452\n",
      "14        0.000001  0.660333                     0.026455\n",
      "15        0.000001  0.770222                     0.026456\n",
      "16        0.000001  0.220778                     0.026457\n",
      "17        0.000001  0.880111                     0.026463\n",
      "18        0.000001  0.110889                     0.026469\n",
      "19        0.000001  0.990000                     0.026477\n",
      "20        0.000001  0.001000                     0.026498\n",
      "21        1.000000  0.110889                     0.063753\n",
      "22        1.000000  0.220778                     0.118879\n",
      "23        1.000000  0.330667                     0.163694\n",
      "24  1000000.000000  0.770222                     0.165109\n",
      "25  1000000.000000  0.660333                     0.165109\n",
      "26  1000000.000000  0.550444                     0.165109\n",
      "27  1000000.000000  0.440556                     0.165109\n",
      "28  1000000.000000  0.330667                     0.165109\n",
      "29  1000000.000000  0.220778                     0.165109\n",
      "30  1000000.000000  0.110889                     0.165109\n",
      "31  1000000.000000  0.001000                     0.165109\n",
      "32     1000.000000  0.990000                     0.165109\n",
      "33     1000.000000  0.880111                     0.165109\n",
      "34     1000.000000  0.770222                     0.165109\n",
      "35     1000.000000  0.660333                     0.165109\n",
      "36        1.000000  0.440556                     0.165109\n",
      "37     1000.000000  0.440556                     0.165109\n",
      "38     1000.000000  0.330667                     0.165109\n",
      "39     1000.000000  0.220778                     0.165109\n",
      "40     1000.000000  0.110889                     0.165109\n",
      "41     1000.000000  0.001000                     0.165109\n",
      "42        1.000000  0.990000                     0.165109\n",
      "43        1.000000  0.880111                     0.165109\n",
      "44        1.000000  0.770222                     0.165109\n",
      "45        1.000000  0.660333                     0.165109\n",
      "46        1.000000  0.550444                     0.165109\n",
      "47  1000000.000000  0.880111                     0.165109\n",
      "48     1000.000000  0.550444                     0.165109\n",
      "49  1000000.000000  0.990000                     0.165109\n",
      "Best alpha: 0.001\n",
      "Best l1_ratio: 0.7702222222222221\n",
      "Train MSE: 0.012404283256205751  R^2: 0.9248319152634302\n",
      "Test  MSE: 0.02830032951068239  R^2: 0.8067484023367857\n"
     ]
    }
   ],
   "source": [
    "# Elastic net\n",
    "\n",
    "# Grids for alpha and ratio\n",
    "alphas = np.logspace(-6, 6, 5)\n",
    "l1_ratios = np.linspace(0.001, 0.99, 10)\n",
    "\n",
    "# initiate the cross validation over alphas and ratios\n",
    "enet = ElasticNet(fit_intercept=True)\n",
    "gridsearch_enet = GridSearchCV(enet, {'alpha': alphas, 'l1_ratio': l1_ratios}, scoring='neg_mean_squared_error', cv=8)\n",
    "gridsearch_enet.fit(X_train_enc_np, y_train)\n",
    "\n",
    "# Dataframe with results MSE\n",
    "cv_df_enet = (\n",
    "    pd.DataFrame({\n",
    "        \"Alpha\": gridsearch_enet.cv_results_[\"param_alpha\"].data.astype(float),\n",
    "        \"l1_ratio\": gridsearch_enet.cv_results_[\"param_l1_ratio\"].data.astype(float),\n",
    "        \"8-fold CV error (MSE-based)\": -gridsearch_enet.cv_results_[\"mean_test_score\"]\n",
    "    })\n",
    "    .sort_values(\"8-fold CV error (MSE-based)\", ascending=True, ignore_index=True)\n",
    ")\n",
    "print(cv_df_enet)\n",
    "\n",
    "# Best alpha and ratio\n",
    "best_params = gridsearch_enet.best_params_\n",
    "print(\"Best alpha:\", best_params[\"alpha\"])\n",
    "print(\"Best l1_ratio:\", best_params[\"l1_ratio\"])\n",
    "\n",
    "#quick check\n",
    "yhat_tr_enet = gridsearch_enet.best_estimator_.predict(X_train_enc_np)\n",
    "yhat_te_enet = gridsearch_enet.best_estimator_.predict(X_test_enc_np)\n",
    "\n",
    "print(\"Train MSE:\", mean_squared_error(y_train, yhat_tr_enet),\n",
    "      \" R^2:\", r2_score(y_train, yhat_tr_enet))\n",
    "print(\"Test  MSE:\", mean_squared_error(y_test,  yhat_te_enet),\n",
    "      \" R^2:\", r2_score(y_test,  yhat_te_enet))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46159ec3",
   "metadata": {},
   "source": [
    "#### Comparison of Regularization Methods\n",
    "\n",
    "The four regularization methods showed quite different results on the full Housing dataset with 263 features. Ridge, Lasso, and Elastic Net all performed very similarly on the log-transformed target, with test R around 0.800.81 and test MSE around 0.0280.029. Specifically, Ridge had R = 0.8067 (MSE = 0.0283), Lasso had R = 0.8051 (MSE = 0.0285), and Elastic Net also had R = 0.8067 (MSE = 0.0283). Their training R values were around 0.920.93, showing that regularization kept the models flexible but still able to generalize well to unseen data.\n",
    "\n",
    "In contrast, the Truncated Pseudoinverse performed extremely poorly, with test R = 8.35 and MSE = 1.37 on the log scale, even though training performance was good (R = 0.94, MSE = 0.010). The threshold chosen by cross-validation (c = 0.376) kept 244 out of 264 singular values, meaning almost no regularization was applied. Testing higher thresholds (c = 10 and c = 50) only made the results worse, showing that the method itself doesnt work well for this dataset.\n",
    "\n",
    "The poor results of the Truncated Pseudoinverse likely come from several issues: possible data leakage during preprocessing, no clear separation between important and noisy singular values, and rank deficiency in the design matrix (rank 253 vs. 263 features). These problems make the matrix numerically unstable and prevent the method from finding a good cutoff. On the other hand, Ridge, Lasso, and Elastic Net handle multicollinearity much better because they shrink coefficients continuously rather than cutting them off abruptly.\n",
    "\n",
    "After transforming predictions back to the original SalePrice scale, all methods performed worse due to the exponential function amplifying errors. Ridge achieved test R = 0.11 (MSE = 5.9710), while the Truncated Pseudoinverse dropped even further to R = 2.34 (MSE = 1.7910). The similar and stable performance of Ridge, Lasso, and Elastic Net confirms that regularization is the right approach for this high-dimensional, correlated dataset, while the Truncated Pseudoinverse is not suitable here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e79cfd6",
   "metadata": {},
   "source": [
    "**c)** During cross-validation, what are possible sources of *information leakage*? Briefly describe what leakage means in this context, and explain what steps you would take to avoid it if you were building a more complete data preprocessing pipeline (e.g., with imputation, scaling, or encoding). You do not need to implement these steps here, only to explain the idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9069eb9",
   "metadata": {},
   "source": [
    "Information leakage in machine learning and cross-validation happens when information from outside the training set accidentally gets used during training. This can make the model's performance look better during validation than it actually is when tested on truly unseen data. In cross-validation, leakage occurs when the validation set influences the training process through feature engineering or preprocessing steps that were done before splitting the data.\n",
    "\n",
    "To build a pipeline without leakage, we need to make sure that any transformation using data statistics is fit only on the training fold and then applied to the validation fold. The best approach is to use sklearn.pipeline.Pipeline or do all the preprocessing inside the cross-validation loop. This means we first split the data into training and validation folds, then fit all transformations only on the training fold, and finally apply those same transformations to both folds before training and evaluating the model.\n",
    "\n",
    "In our implementation, we did all preprocessing steps (imputation, standardization, and one-hot encoding) on the entire training dataset before starting cross-validation. This creates leakage because when we imputed missing values using the mean, that mean was calculated from all observations, including the ones that would later be in validation folds. The same problem happened when we standardized features - the mean and standard deviation came from the full dataset, including validation fold observations. When we created dummy variables through one-hot encoding, the set of categories also came from the entire training set. This means that during each CV fold, the validation data had already influenced the preprocessing, even though we were treating it as unseen data.\n",
    "\n",
    "The correct way to avoid leakage is to make sure any data-dependent transformation is fit using only the training fold, then applied to both training and validation folds. This is important because when preprocessing is done correctly inside the CV loop, each validation fold is truly treated as unseen data. The performance estimates we get will be more realistic and will better represent how the model performs on the actual test set. Data leakage from fitting on information from the validation or test set falsely improves the performance metrics, making the model appear better than it actually is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2a2244",
   "metadata": {},
   "source": [
    "**d)** For the Lasso and Elastic Net regularization techniques, how many coefficients are non-zero ($\\hat{\\beta}_j \\neq 0$)? Compare this number with the number of coefficients retained by the Ridge and Truncated Pseudoinverse models and provide an explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e218fdd",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afce2a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso: 82 non-zero coefficients out of 263\n",
      "\n",
      "Elastic Net: 92 non-zero coefficients out of 263\n",
      "\n",
      "Ridge: 263 non-zero coefficients out of 263\n",
      "\n",
      "Truncated Pseudoinverse: 264 coefficients (kept 244 components out of 264)\n",
      "  All 264 features have non-zero coefficients\n"
     ]
    }
   ],
   "source": [
    "# Count non-zero coefficients for each method\n",
    "\n",
    "# Lasso\n",
    "lasso_coefs = gridsearch_lasso.best_estimator_.coef_\n",
    "lasso_nonzero = np.sum(lasso_coefs != 0)\n",
    "print(f\"Lasso: {lasso_nonzero} non-zero coefficients out of {len(lasso_coefs)}\")\n",
    "\n",
    "# Elastic Net\n",
    "enet_coefs = gridsearch_enet.best_estimator_.coef_\n",
    "enet_nonzero = np.sum(enet_coefs != 0)\n",
    "print(f\"\\nElastic Net: {enet_nonzero} non-zero coefficients out of {len(enet_coefs)}\")\n",
    "\n",
    "# Ridge (all coefficients are non zero)\n",
    "ridge_coefs = gridsearch_ridge.best_estimator_.coef_\n",
    "ridge_nonzero = np.sum(ridge_coefs != 0)  \n",
    "print(f\"\\nRidge: {ridge_nonzero} non-zero coefficients out of {len(ridge_coefs)}\")\n",
    "\n",
    "# Truncated Pseudoinverse - coefficients are in betas_c\n",
    "# It has k non-zero components, but all features get some coefficient\n",
    "trunc_nonzero = len(betas_c) \n",
    "print(f\"\\nTruncated Pseudoinverse: {trunc_nonzero} coefficients (kept {k} components out of {len(D)})\")\n",
    "print(f\"  All {trunc_nonzero} features have non-zero coefficients\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89897c4a",
   "metadata": {},
   "source": [
    "Comparison Across Regularization Methods\n",
    "\n",
    "Lasso Regression (L Penalty)\n",
    "\n",
    "Lasso solves the optimization problem $\\min_{b \\in \\mathbb{R}^{d+1}} \\left( \\|Ab - y\\|_2^2 + \\lambda \\sum_{j=1}^{d} |b_j| \\right)$, where the L-norm penalty $\\sum |b_j|$ creates a constraint region with sharp corners at the coordinate axes. When the objective function's contour intersects this constraint region, it frequently touches at these corners where one or more coefficients become exactly zero. This geometric property makes the L penalty work as both a regularizer and an automatic feature selector. The observed result of 82 non-zero coefficients out of 263 total features demonstrates aggressive feature selection, indicating that the optimal $\\lambda$ was large enough to eliminate about two-thirds of the features as irrelevant for prediction.\n",
    "\n",
    "Elastic Net (Combined L and L Penalty)\n",
    "\n",
    "Elastic Net combines both penalties through $\\min_{b \\in \\mathbb{R}^{d+1}} \\left( \\|Ab - y\\|_2^2 + \\lambda \\left[ \\alpha \\sum_{j=1}^{d} |b_j| + (1-\\alpha) \\sum_{j=1}^{d} b_j^2 \\right] \\right)$, where $\\alpha$ is the mixing parameter. This creates a constraint region that blends the diamond-shaped L region with the circular L region, meaning \"almost\" rounded corners that still induce sparsity but less aggressively than just Lasso. The result of 202 non-zero coefficients out of 263 suggests that the optimal parameters leaned towards moderate regularization. This could be due to a relatively small value of $\\lambda$ or an l1_ratio closer to the L end of the spectrum, which allowed most features to remain in the model.\n",
    "\n",
    "\n",
    "Ridge Regression (L Penalty)\n",
    "\n",
    "Ridge regression has the closed-form solution $\\hat{\\beta}_\\lambda = (A^T A + \\lambda I)^{-1} A^T y = \\sum_{i=1}^{r} \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda} v_i u_i^T y$, where the SVD decomposition $A = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T$ reveals how regularization operates. The L-norm penalty $\\sum b_j^2$ generates a spherical constraint region with no corners. This smooth geometry means the objective function contour almost never intersects the constraint at a point where any coefficient equals exactly zero. Instead, Ridge shrinks each coefficient by the multiplicative factor $\\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda}$, where smaller singular values experience stronger shrinkage. The result is that all 263 coefficients remain non-zero, though features corresponding to small singular values have coefficients approaching zero.\n",
    "\n",
    "Truncated Pseudoinverse\n",
    "\n",
    "The truncated pseudoinverse solution is $A_c^{\\dagger} = \\sum_{i=1}^{k} \\sigma_i^{-1} v_i u_i^T$ where $k = \\#\\{i : \\sigma_i > c\\}$, giving coefficients $\\hat{\\beta}_c = A_c^{\\dagger} y = \\sum_{i=1}^{k} \\sigma_i^{-1} (u_i^T y) v_i$. This method fundamentally differs from the penalty-based approaches because it performs dimension reduction rather than feature selection. While only $k=244$ singular value components are retained (those exceeding threshold $c$), each right singular vector $v_i$ is a linear combination spanning all original feature dimensions. The final coefficient vector therefore assigns non-zero values to all 264 features, as each feature contributes to the retained components. The model operates in a 244-dimensional subspace of the original 264-dimensional feature space, but this subspace involves all feature directions, that is why all features receive non-zero coefficients despite using fewer components than the full rank.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407d68e7",
   "metadata": {},
   "source": [
    "**e)** Based on your findings from Questions 2 and 3, which model would you recommend for predicting house prices? Justify your choice not only by comparing performance metrics, but also by discussing the nature of the problem (e.g., number of features, presence of categorical variables, potential collinearity, sparsity, nonlinearity). Explain how the strengths and limitations of the chosen method align with this problem structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fcd75e",
   "metadata": {},
   "source": [
    "Based on the findings from Questions 2 and 3, the Elastic Net regression on the log-transformed SalePrice is the most appropriate model for predicting house prices.\n",
    "\n",
    "The housing dataset is large and complex, containing over 260 predictors after one-hot encoding. Many of these are categorical features expanded into numerous dummy variables, and several numerical variables are highly collinear-for example, TotalBsmtSF being the sum of its components or similar overlaps among living-area measures. This high-dimensional and correlated feature space makes traditional OLS regression unstable, as seen in the exploding or undefined coefficients and poor generalization once categorical variables were added.\n",
    "\n",
    "The truncated pseudoinverse also failed to produce stable results because the singular values of the design matrix do not show a clear cutoff between signal and noise. As a result, the model either overfitted or lost too much information depending on the threshold, leading to extremely poor out-of-sample accuracy.\n",
    "\n",
    "Regularized models clearly outperformed both OLS and truncated pseudoinverse on both the log-transformed and the original SalePrice scales.\n",
    "\n",
    "On the log scale (where training and cross-validation were performed using MSE as the metric), Ridge, Lasso, and Elastic Net achieved very similar results with test MSE  0.0280.029 and train MSE  0.0100.011, corresponding to R  0.800.81 on the test set. In contrast, the OLS model had a much higher test MSE = 0.1078 (R = 0.26), and the truncated pseudoinverse failed with test MSE = 1.3687 and negative R.\n",
    "\n",
    "When predictions were inverse-transformed back to the original SalePrice scale, performance naturally declined due to the exponential function amplifying residuals. Still, the regularized methods remained the most accurate, with Elastic Net and Ridge reaching test MSE  6.0  10 (R  0.11), while OLS and the truncated pseudoinverse had dramatically worse results-test MSE  1.6  10 (R = 1.85) for OLS and 1.79  10 (R = 2.34) for the truncated pseudoinverse.\n",
    "\n",
    "Among the regularization techniques, Elastic Net provided the most balanced solution. Its L penalty stabilized the correlated groups of predictors, while its L penalty encouraged sparsity and feature selection. This hybrid penalty makes it especially well-suited for data like this, where many features are redundant but still informative in groups.\n",
    "\n",
    "Elastic Net effectively resolves the weaknesses of OLS. It reduces overfitting, handles multicollinearity, and yields stable, interpretable coefficients that reflect key economic factors like neighborhood, quality, and total living area. It generalizes well both on the log scale (low MSE, high R) and after converting predictions back to actual prices (lowest MSE among all tested models).\n",
    "\n",
    "However, the inverse transformation from log to the original scale inevitably increases prediction error for high-priced houses, since percentage-based deviations become larger in dollar terms.\n",
    "\n",
    "Overall, Elastic Net on the log-transformed SalePrice provides the best trade-off between interpretability, robustness, and predictive performance. It minimizes test MSE  0.028 on the log scale and achieves the lowest error on the original SalePrice scale ( 610) among all methods tested. Compared to unregularized OLS and truncated pseudoinverse, it generalizes far better and remains numerically stable, making it the most reliable model for predicting house prices in this dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
